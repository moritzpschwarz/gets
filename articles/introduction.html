<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>An Introduction to the gets package^[This vignette is a minor modification of \citet{PretisReadeSucarrat2018}] • gets</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.4.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.4.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="An Introduction to the gets package^[This vignette is a minor modification of \citet{PretisReadeSucarrat2018}]">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">gets</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.37</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles">
<li><a class="dropdown-item" href="../articles/introduction.html">An Introduction to the gets package^[This vignette is a minor modification of \citet{PretisReadeSucarrat2018}]</a></li>
  </ul>
</li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/gsucarrat/gets/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">



<script src="introduction_files/kePrint-0.0.1/kePrint.js"></script><link href="introduction_files/lightable-0.0.1/lightable.css" rel="stylesheet">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>An Introduction to the gets package^[This vignette is a minor modification of \citet{PretisReadeSucarrat2018}]</h1>
            
            <h4 data-toc-skip class="date">2024-07-26</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/gsucarrat/gets/blob/master/vignettes/introduction.Rmd" class="external-link"><code>vignettes/introduction.Rmd</code></a></small>
      <div class="d-none name"><code>introduction.Rmd</code></div>
    </div>

    
    
<!-- \SweaveOpts{engine=R,eps=FALSE -->
<!-- \VignetteIndexEntry{An introduction to the gets package -->
<!-- \VignetteDepends{gets -->
<!-- \VignetteKeywords{general-to-specific, model selection, variable selection, regression of the mean, regression of the log-variance, time series, AR-X, log-ARCH-X, indicator saturation -->
<!-- \VignettePackage{gets -->
<p>Felix Pretis<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;University of Victoria, Department of Economics &amp;amp;
INET at the Oxford Martin School, University of Oxford. URL: &lt;a href="http://www.felixpretis.org/" class="external-link uri"&gt;http://www.felixpretis.org/&lt;/a&gt;&lt;/p&gt;'><sup>2</sup></a>, J. James Reade<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Programme for Economic Modelling at the Oxford Martin
School &amp;amp; Department of Economics, University of Reading. URL: &lt;a href="https://sites.google.com/site/jjamesreade/" class="external-link uri"&gt;https://sites.google.com/site/jjamesreade/&lt;/a&gt;&lt;/p&gt;'><sup>3</sup></a> and Genaro Sucarrat<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Department of Economics, BI Norwegian Business School.
URL: &lt;a href="https://www.sucarrat.net/" class="external-link uri"&gt;https://www.sucarrat.net/&lt;/a&gt;&lt;/p&gt;'><sup>4</sup></a></p>
<p>This version: 2024-07-26 [Original version: 3 September 2018]</p>
<!-- table of contents -->
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">knitr</span><span class="fu">::</span><span class="va"><a href="https://rdrr.io/pkg/knitr/man/opts_chunk.html" class="external-link">opts_chunk</a></span><span class="op">$</span><span class="fu">set</span><span class="op">(</span></span>
<span>  collapse <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  warning <span class="op">=</span> <span class="cn">FALSE</span>, </span>
<span>  message <span class="op">=</span> <span class="cn">FALSE</span>,</span>
<span>  comment <span class="op">=</span> <span class="st">""</span></span>
<span><span class="op">)</span></span></code></pre></div>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/options.html" class="external-link">options</a></span><span class="op">(</span>rmarkdown.html_vignette.check_title <span class="op">=</span> <span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 class="unnumbered" id="summary">Summary<a class="anchor" aria-label="anchor" href="#summary"></a>
</h2>
<p>This vignette<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;This vignette is a minor modification of &lt;span class="citation"&gt;(&lt;a href="#ref-pretis2018automated"&gt;Felix Pretis,
Reade, and Sucarrat 2018&lt;/a&gt;)&lt;/span&gt;&lt;/p&gt;'><sup>5</sup></a> provides an overview of the R package
<strong>gets</strong>, which contains facilities for automated
general-to-specific (GETS) modeling of the mean and variance of a
regression, and indicator saturation (IS) methods for the detection and
modeling of outliers and structural breaks. The mean can be specified as
an autoregressive model with covariates (an “AR-X” model), and the
variance can be specified as an autoregressive log-variance model with
covariates (a “log-ARCH-X” model). The covariates in the two
specifications need not be the same, and the classical linear regression
model is obtained as a special case when there is no dynamics, and when
there are no covariates in the variance equation. The four main
functions of the package are <code><a href="../reference/arx.html">arx()</a></code>, <code><a href="../reference/getsm.html">getsm()</a></code>,
<code><a href="../reference/getsm.html">getsv()</a></code> and <code><a href="../reference/isat.html">isat()</a></code>. The first function
estimates an AR-X model with log-ARCH-X errors. The second function
undertakes GETS modeling of the mean specification of an
<code>arx</code> object. The third function undertakes GETS modeling of
the log-variance specification of an <code>arx</code> object. The fourth
function undertakes GETS modeling of an indicator-saturated mean
specification allowing for the detection of outliers and structural
breaks. The usage of two convenience functions for export of results to
EViews and STATA are illustrated, and LaTeX code of the estimation
output can readily be generated.</p>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<p>General-to-specific (GETS) modeling combines well-known ingredients:
backwards elimination, single and multiple hypothesis testing,
goodness-of-fit measures and diagnostics tests. The way these are
combined by GETS modeling enables rival theories and models to be tested
against each other, ultimately resulting in a parsimonious,
statistically valid model that explains the characteristics of the data
being investigated. The methodology thus provides a systematic and
coherent approach to model development and maintenance, cumulative
research and scientific progress. This paper provides an overview of the
R package <strong>gets</strong>, which contains facilities for automated
general-to-specific (GETS) modeling of the mean and variance of
cross-sectional and time-series regressions, and indicator saturation
(IS) methods for the detection and modeling of outliers and structural
breaks in the mean.</p>
<p>The origins of GETS modeling can be traced back to Denis Sargan and
the London School of Economics (LSE) during the 1960s, see <span class="citation">(<a href="#ref-hendry2003sargan">Hendry
2003</a>)</span> and <span class="citation">(<a href="#ref-mizon1995progressive">Mizon 1995</a>)</span>. However, it was
not until the 1980s and 1990s that the methodology gained widespread
acceptance and usage in economics, with David F. Hendry in particular
being a main proponent, see the two-volume article collection by <span class="citation">(<a href="#ref-CamposEricssonHendry2005">Campos,
Hendry, and Ericsson 2005</a>)</span> for a comprehensive overview of
the GETS methodology. An important software-contribution to GETS
modeling was made in 1999, when <span class="citation">(<a href="#ref-hoover1999data">Hoover and Perez 1999</a>)</span> re-visited
the data-mining experiment of <span class="citation">(<a href="#ref-lovell1983data">Lovell 1983</a>)</span>. <span class="citation">(<a href="#ref-hoover1999data">Hoover and Perez
1999</a>)</span> showed that automated multi-path GETS modeling
substantially improved upon the then (in economics) popular model
selection strategies. In the study of <span class="citation">(<a href="#ref-hoover1999data">Hoover and Perez 1999</a>)</span>,
purpose-specific but limited MATLAB code was used in the simulations.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;The code is limited in that it allows for a maximum of
10 paths to be searched, and because there is no user manual nor
help-system available. The data and code is available from &lt;a href="http://www.feweb.vu.nl/econometriclinks/journal/volume2/HooverKD_PerezSJ/data_and_code/" class="external-link uri"&gt;http://www.feweb.vu.nl/econometriclinks/journal/volume2/HooverKD_PerezSJ/data_and_code/&lt;/a&gt;.&lt;/p&gt;'><sup>6</sup></a></p>
<p>Subsequently, further improvements were achieved in the commercial
software packages <strong>PcGets</strong> and in its successor
<strong>Autometrics</strong> . In particular, indicator-saturation
methods for the detection of outliers and structural breaks proposed by
<span class="citation">(<a href="#ref-hendry2008automatic">Hendry,
Johansen, and Santos 2008</a>)</span> were added to
<strong>Autometrics</strong> in 2008, see <span class="citation">(<a href="#ref-Doornik2009">Doornik 2009</a>)</span>. Another milestone was
reached in 2011, when the R package <strong>AutoSEARCH</strong> was
published on the Comprehensive R Archive Network (CRAN). The package,
whose code was developed based on <span class="citation">(<a href="#ref-sucarrat2012automated">Sucarrat and Escribano
2012</a>)</span>, offered automated GETS modeling of conditional
variance specifications within the log-ARCH-X class of models. The R
package <strong>gets</strong>, available from CRAN since October 2014,
is the successor of <strong>AutoSEARCH</strong>. The
<strong>gets</strong> package, at the time of writing, is the only
statistical software that offers GETS modeling of the conditional
variance of a regression, in addition to GETS modeling of the mean of a
regression, and indicator saturation (IS) methods for the detection of
breaks of outliers structural breaks in the mean of a regression using
impulses (IIS), step (SIS; see ) as well as trend indicators (TIS).</p>
<p>This paper provides an overview of the <strong>gets</strong> package.
The main model class under consideration is the autoregressive (AR)
model with exponential autoregressive conditional heteroscedastic (ARCH)
variance, possibly with additional covariates in the mean or variance
equations, or in both. In short, the AR-X model with a log-ARCH-X error
term, where the “X” refers to the covariates (the covariates need not be
the same in the mean and variance specifications). It should be
underlined, however, that <strong>gets</strong> is not limited to
time-series models (see Section <a href="#development">2.3</a>): Static
models (e.g., cross-sectional or panel) can be estimated by specifying
the regression without dynamics. The next section, Section <a href="#sec:an:overview">2</a>, provides an overview of GETS modeling and
its alternatives, and outlines the principles that guides the
development of <code>gets</code>. Section <a href="#sec:setting:time:series:attributes">3</a> contains a note on the
advantage of providing the data with time-series attributes – if the
data are indeed time-series, since this is useful for the estimation of
dynamic models, output and graphing. Section <a href="#sec:ar-x:model:with:log-arch-x:errors">4</a> contains an overview
of the AR-X model with log-ARCH-X errors, explains how it can be
simulated, and illustrates how it can be estimated with the
<code>arx</code> function. Section <a href="#sec:gets:model:selection">5</a> illustrates how GETS modeling can
be undertaken with the <code>getsm</code> and <code>getsv</code>
functions. The first undertakes GETS modeling of the mean specification,
whereas the second undertakes GETS modeling of the log-variance
specification. Section <a href="#sec:indicator:saturation">6</a>
introduces the <code>isat</code> function for indicator saturation
methods. Section <a href="#sec:eviews:and:stata:export">7</a>
illustrates how two convenience functions, <code>eviews</code> and
<code>stata</code>, facilitate GETS modeling by users of EViews or STATA
, i.e., the two most popular commercial software packages in
econometrics. The section also briefly alludes to how estimation output
can readily be converted into LaTeX code. Finally, Section <a href="#sec:conclusions">8</a> concludes.</p>
</div>
<div class="section level2">
<h2 id="sec:an:overview">An overview, alternatives and development principles<a class="anchor" aria-label="anchor" href="#sec:an:overview"></a>
</h2>
<div class="section level3">
<h3 id="subsec:gets:modeling">GETS modeling<a class="anchor" aria-label="anchor" href="#subsec:gets:modeling"></a>
</h3>
<p>It is convenient to provide an overview of GETS modeling in terms of
the linear regression model</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>x</mi><mrow><mn>1</mn><mi>t</mi></mrow></msub><mo>+</mo><mi>⋯</mi><mo>+</mo><msub><mi>β</mi><mi>k</mi></msub><msub><mi>x</mi><mrow><mi>k</mi><mi>t</mi></mrow></msub><mo>+</mo><msub><mi>u</mi><mi>t</mi></msub><mo>,</mo><mspace width="2.0em"></mspace><mi>t</mi><mo>=</mo><mn>1</mn><mo>,</mo><mn>2</mn><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><mi>n</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>#</mi><mi>e</mi><mi>q</mi><mo>:</mo><mi>l</mi><mi>i</mi><mi>n</mi><mi>e</mi><mi>a</mi><mi>r</mi><mo>−</mo><mi>r</mi><mi>e</mi><mi>g</mi><mi>r</mi><mi>e</mi><mi>s</mi><mi>s</mi><mi>i</mi><mi>o</mi><mi>n</mi><mo>−</mo><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\begin{equation}

y_{t} = \beta_1 x_{1t} + \cdots + \beta_k x_{kt} + u_{t}, \qquad t=1,2,..., n 

(\#eq:linear-regression-model)

\end{equation}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">y_t</annotation></semantics></math>
is the dependent variable, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>β</mi><annotation encoding="application/x-tex">\beta</annotation></semantics></math>’s
are slope coefficients, the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>’s
are the regressors and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mi>t</mi></msub><annotation encoding="application/x-tex">u_t</annotation></semantics></math>
is a zero mean error term. GETS modeling assumes there exists at least
one “local” data generating process (LDGP) nested in
(@ref(eq:linear-regression-model)). By philosophical assumption
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>h</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">the</annotation></semantics></math>
DGP is not contained in the simple model above, see <span class="citation">(<a href="#ref-sucarrat2010econometric">Sucarrat
2010</a>)</span> and . The qualifier`local’’ thus means it is assumed
that there exists a specification within
(@ref(eq:linear:regression:model)) that is a statistically valid
representation of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mi>h</mi><mi>e</mi></mrow><annotation encoding="application/x-tex">the</annotation></semantics></math>
DGP. Henceforth, for notational and theoretical convenience, we will
assume there exists only a single LDGP, but this is not a necessary
condition.</p>
<p>A variable
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>j</mi><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">x_{jt}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>j</mi><mo>∈</mo><mo stretchy="false" form="prefix">{</mo><mn>1</mn><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><mi>k</mi><mo stretchy="false" form="postfix">}</mo></mrow><annotation encoding="application/x-tex">j\in \{1,...,k\}</annotation></semantics></math>,
is said to be relevant if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>≠</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_j \neq 0</annotation></semantics></math>
and irrelevant if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>β</mi><mi>j</mi></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\beta_j = 0</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}} \geq 0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{irr}} \geq 0</annotation></semantics></math>
denote the number of relevant and irrelevant variables, respectively,
such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>+</mo><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo>=</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">k_{\text{rel}} + k_{\text{irr}} = k</annotation></semantics></math>.
Of course, both
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><annotation encoding="application/x-tex">k_{\text{rel}}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">k_{\text{irr}}</annotation></semantics></math>
are unknown to the investigator. GETS modeling aims at finding a
specification that contains as many relevant variables as possible, and
a proportion of irrelevant variables that corresponds to the
significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
chosen by the investigator. Put differently, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">rel</mtext></msub><annotation encoding="application/x-tex">\widehat{k}_{\text{rel}}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">\widehat{k}_{\text{irr}}</annotation></semantics></math>
are the retained number of relevant and irrelevant variables,
respectively, then GETS modeling aims at satisfying</p>
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">rel</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>→</mo><mspace width="1.0em"></mspace><mn>1</mn><mspace width="1.0em"></mspace><mtext mathvariant="normal">and</mtext><mspace width="1.0em"></mspace><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>→</mo><mi>α</mi><mspace width="1.0em"></mspace><mtext mathvariant="normal">as</mtext><mspace width="1.0em"></mspace><mi>n</mi><mo>→</mo><mi>∞</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">\begin{equation}
E(\widehat{k}_{\text{rel}}/k_{\text{rel}}) \rightarrow \quad 1 \quad \text{and} \quad E(\widehat{k}_{\text{irr}}/k_{\text{irr}}) \rightarrow \alpha \quad \text{as} \quad n\rightarrow\infty,
\end{equation}</annotation></semantics></math><p>when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>,</mo><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}},k_{\text{irr}}&gt;0</annotation></semantics></math>.
If either
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}}=0</annotation></semantics></math>
or
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{irr}}=0</annotation></semantics></math>,
then the criteria are modified in the obvious ways: If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}}=0</annotation></semantics></math>,
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">rel</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">E(\widehat{k}_{\text{rel}}) = 0</annotation></semantics></math>,
and if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{irr}}=0</annotation></semantics></math>,
then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">E(\widehat{k}_{\text{irr}}) = 0</annotation></semantics></math>.
The proportion of spuriously retained variables, i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub></mrow><annotation encoding="application/x-tex">\widehat{k}_{\text{irr}}/k_{\text{irr}}</annotation></semantics></math>,
is also referred to as <em>gauge</em> in the GETS literature, with
distributional results on the gauge for a specific case (the variables
being impulses as in IIS) provided in <span class="citation">(<a href="#ref-johansen2016asymptotic">Johansen and Nielsen
2016</a>)</span>. The relevance proportion, i.e.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub></mrow><annotation encoding="application/x-tex">\widehat{k}_{\text{irr}}/k_{\text{irr}}</annotation></semantics></math>,
is also referred to as <em>potency</em> in the GETS literature.</p>
<p>Table @ref(table:comparison-of-gets-algorithms) contains a comparison
of the variable selection properties of GETS software packages for some
well-known experiments. As the results show, <strong>gets</strong>
performs as expected in the experiments, since the irrelevance
proportion corresponds well to the nominal regressor significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
and since the relevance proportion is 1. Additional simulations, and
comparisons against alternative algorithms, are contained in Section
@ref(subsec:comparison-of-gets-with-alternatives).</p>
<table class="table table table-striped" style="width: auto !important; margin-left: auto; margin-right: auto;">
<caption>

</caption>
<thead>
<tr>
<th style="empty-cells: hide;border-bottom:hidden;" colspan="4">
</th>
<th style="border-bottom:hidden;padding-bottom:0; padding-left:3px;padding-right:3px;text-align: center; " colspan="4">
<div style="border-bottom: 1px solid #ddd; padding-bottom: 5px; ">
Metrics
</div>
</th>
</tr>
<tr>
<th style="text-align:left;">
Experiment
</th>
<th style="text-align:left;">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><annotation encoding="application/x-tex">k_{\text{rel}}</annotation></semantics></math>
</th>
<th style="text-align:left;">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">k_{\text{irr}}</annotation></semantics></math>
</th>
<th style="text-align:left;">
Algorithm
</th>
<th style="text-align:left;">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
</th>
<th style="text-align:left;">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">rel</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\widehat{k}_{\text{rel}}/k_{\text{rel}})</annotation></semantics></math>
</th>
<th style="text-align:left;">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\widehat{k}_{\text{irr}}/k_{\text{irr}})</annotation></semantics></math>
</th>
<th style="text-align:left;">
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>p</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">DGP</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\widehat{p}(\text{DGP})</annotation></semantics></math>
</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left;">
HP1
</td>
<td style="text-align:left;">
0
</td>
<td style="text-align:left;">
40
</td>
<td style="text-align:left;">
<strong>gets</strong>
</td>
<td style="text-align:left;">
139
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.053
</td>
<td style="text-align:left;">
0.269
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
AutoSEARCH
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.049
</td>
<td style="text-align:left;">
0.239
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
HP1999
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.045
</td>
<td style="text-align:left;">
0.292
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
PcGets
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
≈ 0.04
</td>
<td style="text-align:left;">
≈ 0.45
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
HP2
</td>
<td style="text-align:left;">
1
</td>
<td style="text-align:left;">
39
</td>
<td style="text-align:left;">
<strong>gets</strong>
</td>
<td style="text-align:left;">
139
</td>
<td style="text-align:left;">
1.000
</td>
<td style="text-align:left;">
0.056
</td>
<td style="text-align:left;">
0.254
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
AutoSEARCH
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1.000
</td>
<td style="text-align:left;">
0.050
</td>
<td style="text-align:left;">
0.252
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
HP1999
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1.000
</td>
<td style="text-align:left;">
0.107
</td>
<td style="text-align:left;">
0.000
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
PcGets
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
≈ 0.97
</td>
<td style="text-align:left;">
≈ 0.05
</td>
<td style="text-align:left;">
≈ 0.32
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Autometrics
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1.000
</td>
<td style="text-align:left;">
0.063
</td>
<td style="text-align:left;">
0.119
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
<tr>
<td style="text-align:left;">
HP7’
</td>
<td style="text-align:left;">
3
</td>
<td style="text-align:left;">
37
</td>
<td style="text-align:left;">
<strong>gets</strong>
</td>
<td style="text-align:left;">
138
</td>
<td style="text-align:left;">
0.999
</td>
<td style="text-align:left;">
0.055
</td>
<td style="text-align:left;">
0.232
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
AutoSEARCH
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
1.000
</td>
<td style="text-align:left;">
0.051
</td>
<td style="text-align:left;">
0.232
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
HP1999
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.967
</td>
<td style="text-align:left;">
0.082
</td>
<td style="text-align:left;">
0.040
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
PcGets
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
≈ 1.00
</td>
<td style="text-align:left;">
≈ 0.04
</td>
<td style="text-align:left;">
≈ 0.37
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
Autometrics
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
0.999
</td>
<td style="text-align:left;">
0.066
</td>
<td style="text-align:left;">
0.111
</td>
</tr>
<tr>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
<td style="text-align:left;">
</td>
</tr>
</tbody>
</table>
<!-- Variable selection properties of GETS algorithms. The --><!-- table is essentially Table 2 in --><!-- \citet[][p.~724]{sucarrat2012automated} augmented by the --><!-- properties of **gets**, see Appendix~\@ref(appendix) for more details --><!-- on the simulations. The variable selection is undertaken --><!-- with a nominal regressor significance level of --><!-- 5\%. $m(\widehat{k}_{\text{rel}}/k_{\text{rel}})$, average --><!-- proportion of relevant variables $\widehat{k}_{\text{rel}}$ --><!-- retained relative to the actual number of relevant variables --><!-- $k_{\text{rel}}$. --><!-- $m(\widehat{k}_{\text{irr}}/k_{\text{irr}})$, average --><!-- proportion of irrelevant variables --><!-- $\widehat{k}_{\text{irr}}$ retained relative to the actual --><!-- number of irrelevant variables $k_{\text{irr}}$ in the --><!-- GUM. $\widehat{p}(\text{DGP})$, proportion of times the --><!-- exact DGP is found. The properties of the HP1999 algorithm --><!-- are from \citet[][Table 4 on p.~179]{hoover1999data}. The --><!-- properties of the \pkg{PcGets} algorithm are from --><!-- \citet[][Figure 1 on p.~C39]{hendry2005properties}, and the --><!-- properties of the \pkg{Autometrics} algorithm are from --><!-- \citet[][Section --><!-- 6]{Doornik2009}.  {#table:comparison-of-gets-algorithms} --><p>GETS modeling combines well-known ingredients from the
model-selection literature: backwards elimination, tests on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>β</mi><mi>j</mi></msub><annotation encoding="application/x-tex">\beta_j</annotation></semantics></math>’s
(both single and multiple hypothesis tests), diagnostics tests, and
fit-measures (e.g., information criteria). Specifically, GETS modeling
may be described as proceeding in three steps:</p>
<ol style="list-style-type: decimal">
<li><p>Formulate a general unrestricted model (GUM) that passes a set of
chosen diagnostic tests.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content="&lt;p&gt;Currently, the standard diagnostic tests available in
&lt;strong&gt;gets&lt;/strong&gt; are tests for serial correlation and ARCH in the
standardized residuals, and a test for non-normality. In addition, the
user may add her or his own test or set of tests via the
&lt;code&gt;user.diagnostics&lt;/code&gt; argument.}&lt;/p&gt;"><sup>7</sup></a> Each non-significant regressor in the GUM
constitutes the starting point of a backwards elimination path, and a
regressor is non-significant if the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>~value
of a two-sided
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test
is lower than the chosen significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.</p></li>
<li><p>Undertake backwards elimination along multiple paths by removing,
one-by-one, non-significant regressors as determined by the chosen
significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
Each removal is checked for validity against the chosen set of
diagnostic tests, and for parsimonious encompassing (i.e., a multiple
hypothesis test) against the GUM.</p></li>
<li><p>Select, among the terminal models, the specification with the
best fit according to a fit-criterion, e.g., the <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz
1978</a>)</span> information criterion.</p></li>
</ol>
<p>For
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
candidate variables, there are
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mn>2</mn><mi>k</mi></msup><annotation encoding="application/x-tex">2^k</annotation></semantics></math>
possible models. As
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
becomes large the number of models becomes computationally infeasible,
thus, a structured search is required. GETS provides such a structured
search by starting with a general model (the GUM), and subsequently
removing variables along search paths while checking the diagnostics at
each removal.</p>
</div>
<div class="section level3">
<h3 id="subsec:comparison-of-gets-with-alternatives">A comparison of GETS and gets with alternatives<a class="anchor" aria-label="anchor" href="#subsec:comparison-of-gets-with-alternatives"></a>
</h3>
<p>When comparing the R package <strong>gets</strong> to alternatives,
it is important to differentiate the methodological approach of GETS
modeling relative to other modeling approaches, from different software
implementations within the GETS methodology. Here, we denote the broader
field of GETS modeling by GETS, and the R package by
<strong>gets</strong>. First we briefly review and compare alternative
approaches to GETS modeling, then we discuss alternative implementations
of GETS.</p>
<div class="section level4">
<h4 id="sec:GETSalt">GETS compared to alternative methods – a feature-based
comparison<a class="anchor" aria-label="anchor" href="#sec:GETSalt"></a>
</h4>
<p>Numerous model and variable selection methods have been proposed, and
an even larger number of implementations are available. Focusing on
variable selection, Table @ref(table:comparison:of:softwares) contains a
feature-based comparison of <strong>gets</strong> against some common
alternatives in R. The <code>ar</code> function in
<strong>stats</strong> searches for the best
AR(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>)
model using the AIC. The <code>step</code> function, also in
<strong>stats</strong>, offers both forward and backward step-wise
search. The packages <strong>lars</strong> and <strong>glmnet</strong> ,
provide shrinkage-based search methods for variable selection.</p>
<p>As is clear from the table, GETS may be viewed as being more general
than many of its competitors. This comes at a cost: computational speed.
Relying on multiple path searches implies that the required
computational time increases non-linearly with the number of potential
candidate regressors selected over. This is a particular concern when
using indicator saturation <a href="sec:indicator:saturation">see
section “Indicator Saturation”</a>, where the number of candidate
variables scales linearly with the number of observations and
subsequently implies a non-linear increase in required computational
time. For example, selection over
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
(irrelevant) candidate regressors in <strong>gets</strong> (in a sample
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>200</mn></mrow><annotation encoding="application/x-tex">n=200</annotation></semantics></math>
observations) on a 1.8GHz processor requires approximately 0.8 seconds
(s) for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">k=10</annotation></semantics></math>,
2.9s for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">k=20</annotation></semantics></math>,
15s for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>40</mn></mrow><annotation encoding="application/x-tex">k=40</annotation></semantics></math>,
and 114s for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">k=80</annotation></semantics></math>.
By contrast, the identical experiment with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>80</mn></mrow><annotation encoding="application/x-tex">k=80</annotation></semantics></math>
requires 0.16s using the Lasso in <strong>glmnet</strong>, 0.41s in
<strong>lars</strong>, and 0.3s using <code>step</code> (backward).</p>
</div>
<div class="section level4">
<h4 id="gets-compared-to-alternative-methods-a-performance-based-comparison">GETS compared to alternative methods – a performance-based
comparison<a class="anchor" aria-label="anchor" href="#gets-compared-to-alternative-methods-a-performance-based-comparison"></a>
</h4>
<p> together with <span class="citation">(<a href="#ref-castle2011evaluating">Castle, Doornik, and Hendry
2011</a>)</span> provide a broad overview of the performance of GETS
relative to alternative model selection strategies of the mean of a
regression, including step-wise regression, information criteria and
penalized shrinkage-based selection using the Lasso . <span class="citation">(<a href="#ref-castle2015detecting">Castle et al.
2015</a>)</span> compare GETS in the context of step-shifts against the
Lasso using LARS , and <span class="citation">(<a href="#ref-pretis_volc16">F. Pretis and Volz 2016</a>)</span> compare
GETS against the Lasso for designed break functions (see Section <a href="sec:isat_comp">7.3</a>) for a more detailed discussion of
<strong>gets</strong> in the context of break detection). In both
instances shrinkage-based selection is implemented using the R packages
<strong>lars</strong> and <strong>glmnet</strong> . The emerging
consensus from these simulation comparisons is that the false-positive
rate, or irrelevance proportion or gauge, is erratic and difficult to
control in step-wise as well as shrinkage-based selection procedures.
When selecting on information criteria only, the implicit significance
level of selection results in a high gauge when the number of candidate
variables increases relative to the sample size. In contrast, the gauge
tends to be well-calibrated around the nominal size of selection
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
in GETS. While the retention of relevant variables often is high in
shrinkage-based approaches (and erratic in step-wise regression), this
result comes at the cost of a high gauge and the performance becomes
less reliable in the presence of correlation between the candidate
variables.</p>
<p>To provide additional comparisons of performance to alternative
methods for detecting relevant and discarding irrelevant variables, here
we compare <strong>gets</strong> to: shrinkage-based selection, 1-cut
selection (where all variables with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>~values
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≤</mo><mi>α</mi></mrow><annotation encoding="application/x-tex">\leq \alpha</annotation></semantics></math>
in the GUM are retained in a single decision), and conducting selection
inference starting at the DGP itself. The results are provided in Figure
@ref(fig_lass) (and Tables @ref(tab_lassuncorr), @ref(tab_lassposcorr),
and @ref(tab_lassnegcorr) in Appendix @ref(sec:simulation-tables)). The
simulations cover three correlation structures of regressors: First,
in-expectation uncorrelated regressors, second, positively correlated
regressors
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\rho=0.5</annotation></semantics></math>),
and third, alternating negatively correlated regressors (where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\rho(x_i, x_{i+1}) = 0.5</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ρ</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>,</mo><msub><mi>x</mi><mrow><mi>i</mi><mo>+</mo><mn>2</mn></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mn>0.5</mn></mrow><annotation encoding="application/x-tex">\rho(x_i, x_{i+2})=-0.5</annotation></semantics></math>).
We consider a total of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mn>20</mn></mrow><annotation encoding="application/x-tex">k=20</annotation></semantics></math>
regressors in a sample of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>500</mn></mrow><annotation encoding="application/x-tex">n=500</annotation></semantics></math>
observations for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1000</mn><annotation encoding="application/x-tex">1000</annotation></semantics></math>
replications. The number of relevant regressors is increased from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}}=0</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}}=10</annotation></semantics></math>
with coefficients set to correspond to an expected
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-statistic
of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">\approx 3</annotation></semantics></math>.
The performance of <strong>gets</strong> using the <code>getsm</code>
function is compared to the cross-validated Lasso in
<strong>glmnet</strong> and the Lasso with fixed penalty parameter such
that the false-detection rate approximately matches <code>getsm</code>
under the null (when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">k_{\text{rel}}=0</annotation></semantics></math>).
The significance level of 1-cut selection is chosen to match
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mn>1</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha=1\%</annotation></semantics></math>
in <code>getsm</code> selection.</p>
<div class="float">
<embed src="fig_getsm_performance_v2.pdf" style="width:100.0%"></embed><div class="figcaption">test</div>
</div>
<p>The simulation results presented here match the evidence from
previous studies: GETS selection yields a false-detection rate close to
the nominal size of selection regardless of the correlation structure of
regressors considered. While exhibiting high potency, the false
detection rate of Lasso is difficult to control when the correlation
structure varies and the number of relevant variables is unknown. GETS
dominates 1-cut selection when regressors are correlated, and closely
matches 1-cut in absence of correlation.</p>
<p>To the best of our knowledge, the only currently publicly available
software that provides automated model selection of the variance is
<strong>gets</strong>. The reason for this is that <strong>gets</strong>
sidesteps the numerical estimation difficulties usually associated with
models of the variance thanks to its OLS estimation procedure, see the
discussion in <span class="citation">(<a href="#ref-sucarrat2012automated">Sucarrat and Escribano
2012</a>)</span>.</p>
</div>
<div class="section level4">
<h4 id="getsalt">Alternatives within the field of GETS<a class="anchor" aria-label="anchor" href="#getsalt"></a>
</h4>
<p>There have been different software implementations of GETS modeling –
Table @ref(table:feature-comparison:of:gets:softwares) summarizes the
similarities and differences between these. The main (currently
available) alternative to the package <strong>gets</strong> for GETS
modeling of the mean in regression models is
<strong>Autometrics</strong> written in Ox within the software package
<strong>PcGive</strong> . <strong>Autometrics</strong> and
<strong>gets</strong> share common features in GETS modeling of the mean
in regression models, and in the general implementation of impulse- and
step-indicator saturation. There are, however, notable differences
between the two implementations: The main advantages of
<strong>gets</strong> lie in being the only GETS implementation of
variance models, the implementation of new and unique features in
indicator saturation methods including trend-indicator saturation (TIS),
consistency and efficiency corrections of the variance estimates, and
testing of the time-varying mean (see Section <a href="sec:isat_comp">7.3</a>) for an in-depth discussion (see
@ref(isat_comp) of the differences in indicator saturation between
<strong>Autometrics</strong> and <strong>gets</strong>), as well as new
features in model selection (e.g., the availability of a direct function
to correct for model-selection bias). In turn, selection over systems of
equations can be conducted automatically in <strong>Autometrics</strong>
while having to be done by one-equation at a time in
<strong>gets</strong>.</p>
</div>
</div>
<div class="section level3">
<h3 id="development">Development principles of the package gets<a class="anchor" aria-label="anchor" href="#development"></a>
</h3>
<p>The original motivation behind the precursor of <strong>gets</strong>
(i.e., <strong>AutoSEARCH</strong> ) was to make GETS modeling methods
of the variance (and mean) of a regression freely and publicly
available, while being open-source and implementing recent developments
in GETS. This principle will continue to guide the development of
<strong>gets</strong>. Indicator saturation methods were added to
<strong>gets</strong> in version 0.2, and we plan to expand
<strong>gets</strong> further to include model classes for which there
currently is no GETS software, e.g., spatial models, panel-data, etc.
Naturally, we encourage others keen to develop and publish GETS modeling
methods for a wider range of alternatives, either within the
<strong>gets</strong> package or as a separate package. Another
important development principle is that we would like to enable more
user-specified control. User-specified diagnostics, for example, were
added in version 0.10, and we also plan to enable user-specified
estimation and inference procedures (this is already available in
<code>arx</code>, but not in <code>getsm</code>, <code>getsv</code> and
<code>isat</code>). Finally, we also aim at making the package
computationally faster and more user-friendly.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:setting:time:series:attributes">Setting time-series attributes<a class="anchor" aria-label="anchor" href="#sec:setting:time:series:attributes"></a>
</h2>
<p>The <strong>gets</strong> package is not limited to time series
models and does not require that time-series characteristics are set
beforehand (for example if the data at hand are not time series).
However, if time series characteristics are not set, and if the data are
in fact time series, then graphs and other outputs (e.g., fitted values,
residuals, etc.) are not optimal. The <strong>gets</strong> package is
optimized to work with Z’s ordered observations (ZOO) package
<strong>zoo</strong>, see <span class="citation">(<a href="#ref-zeileis2005zoo">Zeileis and Grothendieck 2005</a>)</span>. In
fact, the fitted values, residuals, recursive estimates and so on
returned by <strong>gets</strong> functions, are all objects of class
‘<code>zoo</code>’. The <strong>zoo</strong> package provides a very
general and versatile infrastructure for observations that are ordered
according to an arbitrary index, e.g., time-series, and
<strong>zoo</strong> is adapted to interact well with the less versatile
time-series class of the <strong>base</strong> distribution,
‘<code>ts</code>’: To convert ‘<code>ts</code>’ objects to
‘<code>zoo</code>’ objects, simply use <code>as.zooreg</code>
(preferred) or <code>as.zoo</code>. See the help system and webpage of
the <strong>zoo</strong> package for several short intros and vignettes:
<a href="https://CRAN.R-project.org/package=zoo" class="external-link uri">https://CRAN.R-project.org/package=zoo</a>.</p>
</div>
<div class="section level2">
<h2 id="sec:ar-x:model:with:log-arch-x:errors">The AR-X model with log-ARCH-X errors<a class="anchor" aria-label="anchor" href="#sec:ar-x:model:with:log-arch-x:errors"></a>
</h2>
<p>The specifications considered by <strong>gets</strong> are all
contained in the AR-X model with log-ARCH-X errors. This model is made
up of two equations, one for the mean and one for the log-variance:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>ϕ</mi><mn>0</mn></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>r</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><msub><mi>ϕ</mi><mi>r</mi></msub><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mi>r</mi></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>s</mi><mo>=</mo><mn>1</mn></mrow><mi>S</mi></munderover><msub><mi>η</mi><mi>s</mi></msub><msubsup><mi>x</mi><mrow><mi>s</mi><mo>,</mo><mi>t</mi></mrow><mi>m</mi></msubsup><mo>+</mo><msub><mi>ϵ</mi><mi>t</mi></msub><mo>,</mo><mspace width="2.0em"></mspace><msub><mi>ϵ</mi><mi>t</mi></msub><mo>=</mo><msub><mi>σ</mi><mi>t</mi></msub><msub><mi>z</mi><mi>t</mi></msub><mo>,</mo><mspace width="1.0em"></mspace><msub><mi>z</mi><mi>t</mi></msub><mo>∼</mo><mtext mathvariant="normal">iid</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo></mrow><annotation encoding="application/x-tex">y_t = \phi_0 + \sum_{r=1}^R \phi_r y_{t-r} + \sum_{s=1}^S\eta_s x_{s,t}^m + \epsilon_t, \qquad \epsilon_t =\sigma_tz_t, \quad z_t \sim \text{iid}(0,1),</annotation></semantics></math>
{#eq:ar-x}</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>P</mi></munderover><msub><mi>α</mi><mi>p</mi></msub><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>p</mi></mrow><mn>2</mn></msubsup><mo>+</mo><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>Q</mi></mrow></munder><msub><mi>β</mi><mi>q</mi></msub><mo>ln</mo><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mi>q</mi><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>A</mi></munderover><msub><mi>λ</mi><mi>a</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>a</mi></mrow><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>I</mi><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>a</mi></mrow></msub><mo>&lt;</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><msub><mi>δ</mi><mi>d</mi></msub><msubsup><mi>x</mi><mrow><mi>d</mi><mo>,</mo><mi>t</mi></mrow><mi>v</mi></msubsup></mrow><annotation encoding="application/x-tex">\ln(\sigma_t^2) = \alpha_0 + \sum_{p=1}^P \alpha_p \ln\epsilon_{t-p}^2 + \sum_{q\in Q} \beta_q \ln \text{EqWMA}_{q,t-1} + \sum_{a=1}^A \lambda_a(\ln\epsilon_{t-a}^2)I_{\{\epsilon_{t-a} &lt; 0\}} + \sum_{d=1}^D\delta_d x_{d,t}^v</annotation></semantics></math>
{#eq:log-variance} </p>
<p>The conditional mean equation (@ref(eq:ar-x)) is an autoregressive
(AR) specification of order
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>R</mi><annotation encoding="application/x-tex">R</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>S</mi><annotation encoding="application/x-tex">S</annotation></semantics></math>
covariates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>t</mi></mrow><mi>m</mi></msubsup><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>S</mi><mo>,</mo><mi>t</mi></mrow><mi>m</mi></msubsup></mrow><annotation encoding="application/x-tex">x_{1,t}^m, ..., x_{S,t}^m</annotation></semantics></math>
(“X”), AR-X for short. The covariates may contain lags of conditioning
variables. The error term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ϵ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\epsilon_t</annotation></semantics></math>
is a product of the time-varying conditional standard deviation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>σ</mi><mi>t</mi></msub><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\sigma_t &gt; 0</annotation></semantics></math>
and the real-valued innovation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>
is iid with zero mean and unit variance conditional on the past. The
conditional log-variance equation (@ref(eq:log-variance)) is given by a
logarithmic autoregressive conditional heteroscedasticity (log-ARCH)
specification of order
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>
with volatility proxies defined as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mi>q</mi><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>+</mo><mi>⋯</mi><mo>+</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>q</mi></mrow><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><mi>q</mi></mrow><annotation encoding="application/x-tex">\text{EqWMA}_{q,t-1} = (\epsilon_{t-1}^2 + \cdots + \epsilon_{t-q}^2)/q</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>A</mi><annotation encoding="application/x-tex">A</annotation></semantics></math>
logarithmic asymmetry terms (i.e. “leverage”) analogous to those of
<span class="citation">(<a href="#ref-glosten1993relation">Glosten,
Jagannathan, and Runkle 1993</a>)</span> – so
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>I</mi><mrow><msub><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>a</mi></mrow></msub><mo>&lt;</mo><mn>0</mn></mrow></msub><annotation encoding="application/x-tex">I_{\epsilon_{t-a} &lt; 0}</annotation></semantics></math>
is an indicator function equal to 1 if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>a</mi></mrow></msub><mo>&lt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\epsilon_{t-a} &lt; 0</annotation></semantics></math>
and 0 otherwise, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>D</mi><annotation encoding="application/x-tex">D</annotation></semantics></math>
covariates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>t</mi></mrow><mi>v</mi></msubsup><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mi>D</mi><mo>,</mo><mi>t</mi></mrow><mi>v</mi></msubsup></mrow><annotation encoding="application/x-tex">x_{1,t}^v, ..., x_{D,t}^v</annotation></semantics></math>,
log-ARCH-X for short. The covariates may contain lags of conditioning
variables, and the covariates in the mean need not be the same as those
of the log-variance specification. Hence the superscripts
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>m</mi><annotation encoding="application/x-tex">m</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>v</mi><annotation encoding="application/x-tex">v</annotation></semantics></math>,
respectively. The log-proxies
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mi>q</mi><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">\ln \text{EqWMA}_{q,t-1}</annotation></semantics></math>,
where EqWMA is short for equally weighted moving average, are intended
to proxy lagged log-GARCH terms, e.g.,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><msubsup><mi>σ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\ln\sigma_{t-1}^2</annotation></semantics></math>.
However, it should be noted that the log-proxies can also be given
additional interpretation of interest. For example, if
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>ϵ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t=\epsilon_t</annotation></semantics></math>
is a daily financial return, and if the returns are recorded over
weekdays only, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mn>5</mn><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\text{EqWMA}_{5,t-1}</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mn>20</mn><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\text{EqWMA}_{20,t-1}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mn>60</mn><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><annotation encoding="application/x-tex">\text{EqWMA}_{60,t-1}</annotation></semantics></math>
can be interpreted as the <code>weekly'',</code>monthly’’ and
``quarterly’’ volatilities, respectively. The log-proxies thus provide
great flexibility in modeling the persistence of log-volatility. Also,
note that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mi>q</mi><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>=</mo><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\text{EqWMA}_{q,t-1} = \ln\epsilon_{t-1}^2</annotation></semantics></math>,
i.e., the ARCH(1) term, when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>q</mi><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">q=1</annotation></semantics></math>.
Of course, additional volatility proxies can be included via the
covariates
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>d</mi><mo>,</mo><mi>t</mi></mrow></msub><annotation encoding="application/x-tex">x_{d,t}</annotation></semantics></math>.</p>
<p>The model (@ref(eq:ar-x))–(@ref(eq:log-variance)) is estimated in two
steps.^[A multi-step, iterative procedure might improve the finite
sample efficiency, but does not necessarily improve the asymptotic
efficiency. Joint estimation of the two equations in a single step,
e.g., by Gaussian maximum likelihood, is likely to be asymptotically
more efficient when
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>
is not too fat-tailed, see First, the mean specification (@ref(eq:ar-x))
is estimated by OLS. The default variance-covariance matrix is the
ordinary one, but – optionally – this can be changed to either that of
<span class="citation">(<a href="#ref-white1980heteroskedasticity">White
1980</a>)</span> or that of <span class="citation">(<a href="#ref-newey1987simple">Newey and West 1987</a>)</span>. Second, the
nonlinear AR-representation of (@ref(eq:log-variance)) is estimated,
also by OLS. The nonlinear AR-representation is given by
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="right" style="text-align: right"><mo>ln</mo><msubsup><mi>ϵ</mi><mi>t</mi><mn>2</mn></msubsup></mtd><mtd columnalign="center" style="text-align: center"><mo>=</mo></mtd><mtd columnalign="left" style="text-align: left"><msubsup><mi>α</mi><mn>0</mn><mo>*</mo></msubsup><mo>+</mo><munderover><mo>∑</mo><mrow><mi>p</mi><mo>=</mo><mn>1</mn></mrow><mi>P</mi></munderover><msub><mi>α</mi><mi>p</mi></msub><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>p</mi></mrow><mn>2</mn></msubsup><mo>+</mo><munder><mo>∑</mo><mrow><mi>q</mi><mo>∈</mo><mi>Q</mi></mrow></munder><msub><mi>β</mi><mi>q</mi></msub><mo>ln</mo><msub><mtext mathvariant="normal">EqWMA</mtext><mrow><mi>q</mi><mo>,</mo><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd columnalign="right" style="text-align: right"></mtd><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="left" style="text-align: left"><mo>+</mo><munderover><mo>∑</mo><mrow><mi>a</mi><mo>=</mo><mn>1</mn></mrow><mi>A</mi></munderover><msub><mi>λ</mi><mi>a</mi></msub><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>a</mi></mrow><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><msub><mi>I</mi><mrow><mo stretchy="false" form="prefix">{</mo><msub><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mi>a</mi></mrow></msub><mo>&lt;</mo><mn>0</mn><mo stretchy="false" form="postfix">}</mo></mrow></msub><mo>+</mo><munderover><mo>∑</mo><mrow><mi>d</mi><mo>=</mo><mn>1</mn></mrow><mi>D</mi></munderover><msub><mi>δ</mi><mi>d</mi></msub><msubsup><mi>x</mi><mrow><mi>d</mi><mo>,</mo><mi>t</mi></mrow><mi>v</mi></msubsup><mo>+</mo><msub><mi>u</mi><mi>t</mi></msub><mo>,</mo></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{eqnarray}
\ln\epsilon_t^2 &amp;=&amp; \alpha_0^* + \sum_{p=1}^P \alpha_p \ln\epsilon_{t-p}^2 + \sum_{q\in Q} \beta_q \ln \text{EqWMA}_{q,t-1}\\
%
&amp;&amp; + \sum_{a=1}^A \lambda_a(\ln\epsilon_{t-a}^2)I_{\{\epsilon_{t-a} &lt; 0\}} + \sum_{d=1}^D\delta_d x_{d,t}^v + u_t,
\end{eqnarray}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>α</mi><mn>0</mn><mo>*</mo></msubsup><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mo>+</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>z</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha_0^* = \alpha_0 + E(\ln z_t^2)</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>t</mi></msub><mo>=</mo><mo>ln</mo><msubsup><mi>z</mi><mi>t</mi><mn>2</mn></msubsup><mo>−</mo><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>z</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_t=\ln z_t^2 - E(\ln z_t^2)</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>u</mi><mi>t</mi></msub><mo>∼</mo><mtext mathvariant="normal">iid</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msubsup><mi>σ</mi><mi>u</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">u_t \sim \text{iid}(0, \sigma_u^2)</annotation></semantics></math>.
This provides consistent estimates of all the parameters in
(@ref(eq:log-variance)) except
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>α</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\alpha_0</annotation></semantics></math>,
under appropriate assumptions. To identify
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>α</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\alpha_0</annotation></semantics></math>,
an estimate of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>z</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">E(\ln z_t^2)</annotation></semantics></math>
is needed, which depends on the density of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>.
<span class="citation">(<a href="#ref-sucarrat2016estimation">Sucarrat,
Grønneberg, and Escribano 2016</a>)</span> show that a simple formula
made up of the residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>u</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\widehat{u}_t</annotation></semantics></math>
provides a consistent and asymptotically normal estimate under very
general and non-restrictive assumptions. The estimator is essentially
the negative of the natural log of the smearing estimate of <span class="citation">(<a href="#ref-Duan1983">Duan 1983</a>)</span>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>E</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>z</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mo>−</mo><mo>ln</mo><mrow><mo stretchy="true" form="prefix">[</mo><msup><mi>n</mi><mrow><mo>−</mo><mn>1</mn></mrow></msup><msubsup><mo>∑</mo><mrow><mi>t</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>u</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">\widehat{E}(\ln z_t^2) = -\ln \left[ n^{-1} \sum_{t=1}^n \exp(\widehat{u}_t) \right]</annotation></semantics></math>.
So the expression in square brackets is the smearing estimate. The
log-variance intercept
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>α</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\alpha_0</annotation></semantics></math>
can thus be estimated by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>α</mi><mo accent="true">̂</mo></mover><mn>0</mn><mo>*</mo></msubsup><mo>−</mo><mover><mi>E</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mi>z</mi><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\widehat{\alpha}_0^* - \widehat{E}(\ln z_t^2)</annotation></semantics></math>.
Finally, the ordinary variance-covariance matrix is used for inference
in the log-variance specification, since the error term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>u</mi><mi>t</mi></msub><annotation encoding="application/x-tex">u_t</annotation></semantics></math>
of the nonlinear AR-representation is iid.</p>
<div class="section level3">
<h3 id="subsec:simulation">Simulation<a class="anchor" aria-label="anchor" href="#subsec:simulation"></a>
</h3>
<p>Simulation from an
AR(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>P</mi><annotation encoding="application/x-tex">P</annotation></semantics></math>)
process can readily be done with the <code>arima.sim</code> function in
the <strong>stats</strong> package (part of the base distribution of R).
For example, the following code simulates 100 observations from the
AR(1) model
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><msub><mi>ϕ</mi><mn>0</mn></msub><mo>+</mo><msub><mi>ϕ</mi><mn>1</mn></msub><msub><mi>y</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>ϵ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t = \phi_0 + \phi_1 y_{t-1} + \epsilon_t</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\phi_0=0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mn>1</mn></msub><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">\phi_1=0.4</annotation></semantics></math>:</p>
<p>set.seed(123) y &lt;- arima.sim(list(ar = 0.4), 100)</p>
<p>To simulate from a model with log-ARCH errors, we first need to
simulate the errors. This can be achieved with <code>lgarchSim</code>
from the <strong>lgarch</strong> package :</p>
<p>library(“lgarch”)</p>
<p>Next, the following code simulates an error-term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>ϵ</mi><mi>t</mi></msub><annotation encoding="application/x-tex">\epsilon_t</annotation></semantics></math>
that follows the log-ARCH(1) specification
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mo>+</mo><msub><mi>α</mi><mn>1</mn></msub><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\ln\sigma_t^2 = \alpha_0 + \alpha_1 \ln\epsilon_{t-1}^2</annotation></semantics></math>
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\alpha_0=0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>α</mi><mn>1</mn></msub><mo>=</mo><mn>0.3</mn></mrow><annotation encoding="application/x-tex">\alpha_1=0.3</annotation></semantics></math>:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">eps</span> <span class="op">&lt;-</span> <span class="fu">lgarchSim</span><span class="op">(</span><span class="fl">100</span>, arch <span class="op">=</span> <span class="fl">0.3</span>, garch <span class="op">=</span> <span class="fl">0</span><span class="op">)</span></span></code></pre></div>
<p>By default, the standardized error
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>
is normal, but this can be changed via the <code>innovation</code>
argument of the <code>lgarchSim</code> function. To combine the log-ARCH
error with an AR(1) model with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mn>0</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\phi_0=0</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>ϕ</mi><mn>1</mn></msub><mo>=</mo><mn>0.4</mn></mrow><annotation encoding="application/x-tex">\phi_1=0.4</annotation></semantics></math>
the following code can be used:</p>
<p>yy &lt;- arima.sim(list(ar = 0.4), 100, innov = eps)</p>
<!--  -->
<p>The command <code>plot(as.zoo(cbind(y, yy, eps)))</code> plots the
three series.</p>
</div>
<div class="section level3">
<h3 id="subsec:arx:estimation">arx(): Estimation<a class="anchor" aria-label="anchor" href="#subsec:arx:estimation"></a>
</h3>
<p>The function <code>arx</code> estimates an AR-X model with log-ARCH-X
errors. For example, the following code loads the <strong>gets</strong>
package, fits an AR(1) model with intercept to the series <code>y</code>
generated in Section <a href="subsec:simulation">5.1</a>, and stores the
results in an object called <code>mod01</code>:</p>
<p>library(“gets”) mod01 &lt;- arx(y, ar = 1)</p>
<!--  -->
<p>To print the estimation results, simply type <code>mod01</code>. This
returns:</p>
<p>Date: Fri Aug 06 10:57:59 2021 Dependent var.: y Method: Ordinary
Least Squares (OLS) Variance-Covariance: Ordinary No. of observations
(mean eq.): 99 Sample: 2 to 100</p>
<p>Mean equation:</p>
<p>coef std.error t-stat p-value<br>
mconst 0.034045 0.091664 0.3714 0.7111<br>
ar1 0.397411 0.095212 4.1740 6.533e-05</p>
<p>Diagnostics and fit:</p>
<p>Chi-sq df p-value Ljung-Box AR(2) 0.25922 2 0.8784 Ljung-Box ARCH(1)
0.26124 1 0.6093</p>
<p>SE of regression 0.90933 R-squared 0.15226 Log-lik.(n=99)
-130.06490</p>
<!--  -->
<p>The two diagnostic tests are of the standardized residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>z</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\widehat{z}_t</annotation></semantics></math>.
The AR and ARCH tests are <span class="citation">(<a href="#ref-ljung1978measure">Ljung and Box 1978</a>)</span> tests for
serial correlation in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>z</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\widehat{z}_t</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>z</mi><mo accent="true">̂</mo></mover><mi>t</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\widehat{z}_t^2</annotation></semantics></math>,
respectively, and the number in parentheses indicates at which lag the
test is conducted. <code>R-squared</code> is that of the mean
specification, whereas the (Gaussian) log-likelihood is made up of the
residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>ϵ</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\widehat{\epsilon}_t</annotation></semantics></math>.
If no log-variance specification is fitted, then the conditional
variance in the log-likelihood is constant and equal to the sample
variance of the residuals. By contrast, if a log-variance specification
is fitted, then the conditional variance in the log-likelihood is equal
to the fitted conditional variance, which is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mi>t</mi><mn>2</mn></msubsup><mo>=</mo><mo>exp</mo><mrow><mo stretchy="true" form="prefix">(</mo><mo>ln</mo><msubsup><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mi>t</mi><mn>2</mn></msubsup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\widehat{\sigma}_t^2 = \exp(\ln\widehat{\sigma}_t^2)</annotation></semantics></math>.</p>
<p>The main optional arguments of the <code>arx</code> function when
estimating the mean are:</p>
<ul>
<li><p><code>mc</code>: <code>TRUE</code> (default) or
<code>FALSE</code>. <code>mc</code> is short for ``mean constant’’, so
<code>mc = TRUE</code> includes an intercept, whereas <code>FALSE</code>
does not.</p></li>
<li><p><code>ar</code>: integer vector that indicates the AR terms to
include, say, <code>ar = 1</code>, <code>ar = 1:4</code> or
<code>ar = c(2, 4)</code>.</p></li>
<li><p><code>mxreg</code>: vector, matrix or `<code>zoo</code>’ object
that contains additional regressors to be included in the mean
specification.</p></li>
<li><p><code>vcov.type</code>: the type of variance-covariance matrix
used for inference in the mean specification. By default, the ordinary
(<code>"ordinary"</code>) matrix is used. The other options available
are <code>"white"</code>, i.e., the heteroscedasticity robust
variance-covariance matrix of <span class="citation">(<a href="#ref-white1980heteroskedasticity">White 1980</a>)</span>, and
<code>"newey-west"</code>, i.e., the heteroscedasticity and
autocorrelation robust variance-covariance matrix of <span class="citation">(<a href="#ref-newey1987simple">Newey and West
1987</a>)</span>.</p></li>
</ul>
<p>To make full use of these arguments, let us first generate a set of 5
regressors:</p>
<p>mX &lt;- matrix(rnorm(100 * 5), 100, 5)</p>
<p>Next, the following code estimates an AR-X model with an intercept,
two AR-lags and five regressors, and stores the estimation results in an
object called <code>mod02</code>:</p>
<p>mod02 &lt;- arx(y, ar = 1:2, mxreg = mX, vcov.type = “white”)</p>
<p>Estimation of the log-variance specification is also undertaken with
the <code>arx</code> function. For example, the following code fits the
log-ARCH(1) specification
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mo>+</mo><msub><mi>α</mi><mn>1</mn></msub><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\ln\sigma_t^2 = \alpha_0 + \alpha_1 \ln\epsilon_{t-1}^2</annotation></semantics></math>
to the variable <code>eps</code> generated above:</p>
<p>mod03 &lt;- arx(eps, mc = FALSE, arch = 1)</p>
<p>Typing <code>mod03</code> prints the estimation results. The main
optional arguments when estimating the log-variance are:</p>
<ul>
<li><p><code>arch</code> : integer vector that indicates the log-ARCH
terms to include, say, <code>arch = 1</code>, <code>arch = 1:3</code> or
<code>arch = c(3, 5)</code>.</p></li>
<li><p><code>asym</code>: integer vector that indicates the logarithmic
asymmetry terms (often referred to as ``leverage’’) to include, say,
<code>asym = 1</code>, <code>asym = 1:4</code>, or
<code>asym = c(2, 4)</code>.</p></li>
<li><p><code>vxreg</code>: vector, matrix or `<code>zoo</code>’ object
that contains additional regressors to be included in the log-volatility
specification.</p></li>
</ul>
<p>The following code provides an example that makes use of all three
arguments:</p>
<p>mod04 &lt;- arx(eps, mc = FALSE, arch = 1:3, asym = 2, vxreg =
log(mX^2))</p>
<p>Again, typing <code>mod04</code> prints the results. Finally we give
an example where we jointly fit a mean and log-variance equation to the
series <code>yy</code> generated above, using the variance-covariance
matrix of <span class="citation">(<a href="#ref-white1980heteroskedasticity">White 1980</a>)</span> for the
mean equation:</p>
<p>mod05 &lt;- arx(yy, ar = 1:2, mxreg = mX, arch = 1:3, asym = 2, vxreg
= log(mX^2), vcov.type = “white”)</p>
</div>
<div class="section level3">
<h3 id="subsec:arx:extraction:functions">Extraction functions<a class="anchor" aria-label="anchor" href="#subsec:arx:extraction:functions"></a>
</h3>
<p>There are a number of functions available for extracting information
from ‘<code>arx</code>’ objects. The most important of these (most of
them S3 methods) are:</p>
<p><code>coef, ES, fitted, logLik, plot, predict, print, recursive, residuals,   rsquared, sigma, summary, toLatex, VaR, vcov</code></p>
<p>Six of these (<code>coef</code>, <code>fitted</code>,
<code>predict</code>, <code>recursive</code>,
<code>residuals} and</code>vcov}) have an optional argument that allows
you to choose whether to extract information pertaining to the mean or
log-variance specification. The <code>print</code> function prints the
estimation result, <code>logLik</code> extracts the (Gaussian)
log-likelihood associated with the joint model, <code>summary</code>
lists the entries of the ‘<code>arx</code>’ object (a
<code>list</code>), <code>plot</code> plots the fitted values and
residuals of the model, <code>recursive</code> computes and – optionally
– plots the recursive coefficient estimates, <code>rsquared</code> and
<code>sigma</code> extract the R-squared and standard error of
regression, respectively, while <code>ES</code> and <code>VaR</code>
extract the conditional expected shortfall and value-at-risk,
respectively.</p>
</div>
<div class="section level3">
<h3 id="subsec:arx:example">Example: A model of quarterly inflation with time-varying
conditional variance<a class="anchor" aria-label="anchor" href="#subsec:arx:example"></a>
</h3>
<p>When <span class="citation">(<a href="#ref-Engle82">Engle
1982</a>)</span> proposed the ARCH-class of models, his empirical
application was the uncertainty of UK-inflation. However, the ARCH(4)
specification he used to model the conditional variance was severely
restricted in order to ensure the positivity of the variance estimates,
see . Arguably, this is why (non-exponential) ARCH specifications never
became popular in macroeconomics. The log-ARCH class of models, by
contrast, does not suffer from the positivity problem, since the
conditional variance is specified in logs. To illustrate we fit an
AR(4)-X-log-ARCH(4)-X model to a quarterly inflation series, and show
that the conditional variance specification provides a substantial
improvement in terms of fit and diagnostics.</p>
<p>The following code imports the data<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;The source of the data is Statistics Norway (&lt;a href="http://www.ssb.no/" class="external-link uri"&gt;http://www.ssb.no/&lt;/a&gt;). The
original untransformed data, a monthly consumer price index (CPI), was
retrieved 14 February 2016 via &lt;a href="http://www.ssb.no/tabell/08183/" class="external-link uri"&gt;http://www.ssb.no/tabell/08183/&lt;/a&gt;&lt;/p&gt;'><sup>8</sup></a> and assigns it
quarterly time-series attributes:</p>
<p>data(“infldata”, package = “gets”) infldata &lt;- zooreg(infldata[,
-1], frequency = 4, start = c(1989, 1))</p>
<p>Note that <code>[, -1]</code> removes the first column, since it is
not needed. The dataset thus contains four variables: <code>infl</code>,
<code>q2dum</code>, <code>q3dum</code> and <code>q4dum</code>. The first
variable is quarterly Norwegian inflation (year-on-year) in % from
1989(1) to 2015(4), whereas the latter three are seasonal dummies
associated with the second, third and fourth quarter, respectively.
Initially, to illustrate why a time-varying conditional variance is
needed, we estimate only the mean specification:</p>
<p><span class="math display">$$\begin{equation}
\verb|infl|_t = \phi_0 + \sum_{r=1}^4 \phi_r \verb|infl|_{t-r} + \eta_2
\verb|q2dum|_{t} + \eta_3 \verb|q3dum|_{t} + \eta_4 \verb|q4dum|_{t} +
\epsilon_t
\end{equation}$$</span> </p>
<p>That is, an AR(4)-X, where the dummies constitute the X-part. The
code</p>
<p><code>inflMod01 &lt;- arx(inflData[, "infl"], ar = 1:4, mxreg = inflData[, 2:4], vcov.type = "white")</code></p>
<p>estimates the model using heteroscedasticity-robust coefficient
standard errors of the <span class="citation">(<a href="#ref-white1980heteroskedasticity">White 1980</a>)</span> type, and
typing <code>inflMod01</code> prints the estimation results:</p>
<p>Date: Fri Aug 06 11:11:17 2021 Dependent var.: y Method: Ordinary
Least Squares (OLS) Variance-Covariance: White (1980) No. of
observations (mean eq.): 104 Sample: 1990(1) to 2015(4)</p>
<p>Mean equation:</p>
<p>coef std.error t-stat p-value<br>
mconst 0.8386311 0.2961338 2.8319 0.005637 ar1 0.7257550 0.1300407
5.5810 2.211e-07 ar2 0.0195911 0.1171347 0.1673 0.867523<br>
ar3 0.0350092 0.1385735 0.2526 0.801087<br>
ar4 -0.1676751 0.1336972 -1.2541 0.212836<br>
q2dum -0.0148892 0.2333917 -0.0638 0.949266<br>
q3dum -0.0072972 0.2262704 -0.0322 0.974340<br>
q4dum 0.0103990 0.2226772 0.0467 0.962849</p>
<p>Diagnostics and fit:</p>
<p>Chi-sq df p-value<br>
Ljung-Box AR(5) 16.3205 5 0.005986 Ljung-Box ARCH(1) 5.9665 1
0.014580</p>
<p>SE of regression 0.72814 R-squared 0.53166 Log-lik.(n=104)
-110.57435</p>
<p>The diagnostics suggest the standardized residuals are autocorrelated
and heteroscedastic, since the tests for autocorrelation and
heteroscedasticity yield
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>~values
of 0.6% and 1.5%, respectively. Next, we specify the conditional
variance as a log-ARCH(4)-X, where the X-part is made up of the seasonal
dummies: <span class="math display">$$\begin{equation}
\ln\sigma_t^2 = \alpha_0 + \sum_{p=1}^4 \alpha_p \ln\epsilon_{t-p}^2 +
\delta_2 \verb|q2dum|_{t} + \delta_3 \verb|q3dum|_{t} + \delta_4
\verb|q4dum|_{t}.
\end{equation}$$</span> The code</p>
<p>inflMod02 &lt;- arx(inflData[, “infl”], ar = 1:4, mxreg = inflData[,
2:4], arch = 1:4, vxreg = inflData[, 2:4], vcov.type = “white”)</p>
<p>estimates the full model with <span class="citation">(<a href="#ref-white1980heteroskedasticity">White 1980</a>)</span> standard
errors in the mean and ordinary standard errors in the log-variance.
Typing <code>inflMod02</code> returns</p>
<p>Date: Fri Aug 06 11:12:20 2021 Dependent var.: y Method: Ordinary
Least Squares (OLS) Variance-Covariance: White (1980) No. of
observations (mean eq.): 104 Sample: 1990(1) to 2015(4)</p>
<p>Mean equation:</p>
<p>coef std.error t-stat p-value<br>
mconst 0.8386311 0.2961338 2.8319 0.005637 ar1 0.7257550 0.1300407
5.5810 2.211e-07 ar2 0.0195911 0.1171347 0.1673 0.867523<br>
ar3 0.0350092 0.1385735 0.2526 0.801087<br>
ar4 -0.1676751 0.1336972 -1.2541 0.212836<br>
q2dum -0.0148892 0.2333917 -0.0638 0.949266<br>
q3dum -0.0072972 0.2262704 -0.0322 0.974340<br>
q4dum 0.0103990 0.2226772 0.0467 0.962849</p>
<p>Log-variance equation:</p>
<p>coef std.error t-stat p-value<br>
vconst 0.95935 0.53464 3.2199 0.072749 arch1 0.16697 0.10352 1.6130
0.110169<br>
arch2 0.12027 0.10335 1.1637 0.247566<br>
arch3 0.14740 0.10332 1.4267 0.157060<br>
arch4 0.05982 0.10515 0.5689 0.570824<br>
q2dum -1.32860 0.61862 -2.1477 0.034366 q3dum -0.92707 0.58400 -1.5874
0.115843<br>
q4dum -1.82736 0.62014 -2.9467 0.004069</p>
<p>Diagnostics and fit:</p>
<p>Chi-sq df p-value Ljung-Box AR(5) 9.1776 5 0.1022 Ljung-Box ARCH(5)
1.7613 5 0.8811</p>
<p>SE of regression 0.72814 R-squared 0.53166 Log-lik.(n=100)
-82.32892</p>
<p>The first noticeable difference between <code>inflMod01</code> and
<code>inflMod02</code> is that the diagnostics improve substantially. In
<code>inflMod02</code>, the AR and ARCH tests of the standardized
residuals suggest the standardized error
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>
is uncorrelated and homoscedastic at the usual significance levels (1%,
5% and 10%), and the <span class="citation">(<a href="#ref-JarqueBera1980">Jarque and Bera 1980</a>)</span> test
suggests
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mi>t</mi></msub><annotation encoding="application/x-tex">z_t</annotation></semantics></math>
is normal. The second noticeable improvement is in terms of fit, as
measured by the average (Gaussian) log-likelihood. In
<code>inflMod01</code> the average log-likelihood is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>110.57435</mn><mi>/</mi><mn>104</mn><mo>=</mo><mo>−</mo><mn>1.06</mn></mrow><annotation encoding="application/x-tex">-110.57435/104= -1.06</annotation></semantics></math>,
whereas in <code>inflMod02</code> the average log-likelihood is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>82.3289</mn><mi>/</mi><mn>100</mn><mo>=</mo><mo>−</mo><mn>0.82</mn></mrow><annotation encoding="application/x-tex">-82.3289/100= -0.82</annotation></semantics></math>.
This is a substantial increase. In terms of the <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz
1978</a>)</span> information criterion (SC), which favors parsimony, a
comparison of the average log-likelihoods can be made by the
<code>info.criterion</code> function:</p>
<p>info.criterion(as.numeric(logLik(inflMod01)), n = 104, k = 8 + 1)
info.criterion(as.numeric(logLik(inflMod02)), n = 100, k = 8 + 8)</p>
<p>As is clear, the value falls from 2.53 in <code>inflMod01</code> to
2.38 in <code>inflMod02</code>. (A comparison of the average
log-likelihoods is necessary, since the two models are estimated with a
different number of observations. This is the main difference between
the <code>info.criterion</code> function and <code>AIC</code> and
<code>BIC</code>.) Together, the enhanced fit and diagnostics indicate
the log-variance specification provides a notable improvement. Later, in
Section @ref(subsec:gets:inflation:example), we will undertake GETS
modeling of the mean and variance specifications of
<code>inflMod02</code>.</p>
</div>
<div class="section level3">
<h3 id="subsec:arx:example:sp500-volatility">Example: A log-ARCH-X model of daily SP500 volatility<a class="anchor" aria-label="anchor" href="#subsec:arx:example:sp500-volatility"></a>
</h3>
<p>The most common volatility specification in finance are first order
GARCH-like specifications. In the log-GARCH class of models, this
corresponds to a log-GARCH(1, 1):
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>ln</mo><msubsup><mi>σ</mi><mi>t</mi><mn>2</mn></msubsup><mo>=</mo><msub><mi>α</mi><mn>0</mn></msub><mo>+</mo><msub><mi>α</mi><mn>1</mn></msub><mo>ln</mo><msubsup><mi>ϵ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup><mo>+</mo><msub><mi>β</mi><mn>1</mn></msub><mo>ln</mo><msubsup><mi>σ</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">\ln\sigma_t^2 = \alpha_0 + \alpha_1\ln\epsilon_{t-1}^2 + \beta_1\ln\sigma_{t-1}^2</annotation></semantics></math>.
Here, we show that a log-ARCH-X model that makes use of commonly
available information provides a better fit.</p>
<p>We start by loading a dataset of the Standard and Poor’s 500 (SP500)
index:</p>
<p>data(“sp500data”, package = “gets”) sp500data &lt;- zoo(sp500data[,
-1], order.by = as.Date(sp500data[, “Date”]))</p>
<p>The dataset contains the daily value of the SP500 index, its highs
and lows, and daily volume. We will make use of this information
together with day-of-the-week dummies to construct a rich model of SP500
return volatility. But first we shorten the sample, since not all
variables are available from the start:</p>
<p>sp500data &lt;- window(sp500data, start = as.Date(“1983-07-01”))</p>
<p>The resulting sample thus goes from 1 July 1983 to 8 March 2016, a
total of 8241 observations before differencing and lagging. Next, the
following lines of code create a variable equal to the log-return in
percent, a lagged range-based volatility proxy, and the lagged
log-difference of volume:</p>
<p>sp500Ret &lt;- diff(log(sp500data[, “Adj.Close”])) * 100 relrange
&lt;- (log(sp500data[, “High”]) - log(sp500data[, “Low”]) ) * 100
volproxy &lt;- log(relrange^2) volproxylag &lt;- lag(volproxy, k = -1)
volume &lt;- log(sp500data[, “Volume”]) volumediff &lt;- diff(volume) *
100 volumedifflag &lt;- lag(volumediff, k = -1)</p>
<p>Finally, we make the day-of-the-week dummies and estimate the full
model, a log-ARCH(5)-X specification:</p>
<p>sp500Index &lt;- index(sp500Ret) days &lt;- weekdays(sp500Index) days
&lt;- union(days, days) dTue &lt;- zoo(as.numeric(weekdays(sp500Index)
== days[1]), order.by = sp500Index) dWed &lt;-
zoo(as.numeric(weekdays(sp500Index) == days[2]), order.by = sp500Index)
dThu &lt;- zoo(as.numeric(weekdays(sp500Index) == days[3]), order.by =
sp500Index) dFri &lt;- zoo(as.numeric(weekdays(sp500Index) == days[4]),
order.by = sp500Index) sp500Mod01 &lt;- arx(sp500Ret, mc = FALSE, arch =
1:5, log.ewma = c(5, 20, 60, 120), asym = 1, vxreg = cbind(volproxylag,
volumedifflag, dTue, dWed, dThu, dFri))</p>
<!--  -->
<p>Typing <code>sp500Mod01</code> returns the following print
output:</p>
<p>Date: Fri Aug 06 11:17:38 2021 Dependent var.: y Method: Ordinary
Least Squares (OLS) Sample: 1983-07-05 to 2016-03-08</p>
<p>Log-variance equation:</p>
<p>coef std.error t-stat p-value<br>
vconst 0.0107260 0.0784437 0.0187 0.891241<br>
arch1 -0.0482520 0.0161972 -2.9790 0.002900 arch2 0.0071996 0.0122312
0.5886 0.556127<br>
arch3 0.0256668 0.0122521 2.0949 0.036212 arch4 0.0149581 0.0122145
1.2246 0.220758<br>
arch5 0.0371055 0.0122796 3.0217 0.002521 asym1 -0.0336271 0.0175185
-1.9195 0.054954 logEqWMA(5) 0.0262491 0.0519435 0.5053 0.613334<br>
logEqWMA(20) 0.2817220 0.0713466 3.9486 7.926e-05 logEqWMA(60) 0.1970841
0.1052311 1.8729 0.061122 logEqWMA(120) 0.1936954 0.0865864 2.2370
0.025312 volproxylag 0.2078785 0.0400515 5.1903 2.151e-07 volumedifflag
-0.0030906 0.0014207 -2.1754 0.029630 dTue 0.0978314 0.0834703 1.1720
0.241212<br>
dWed -0.0804053 0.0853471 -0.9421 0.346171<br>
dThu 0.0838896 0.0843500 0.9945 0.319988<br>
dFri 0.0756869 0.0840118 0.9009 0.367664</p>
<p>Diagnostics and fit:</p>
<p>Chi-sq df p-value<br>
Ljung-Box AR(1) 0.53421 1 0.4648<br>
Ljung-Box ARCH(6) 29.21040 6 5.55e-05</p>
<p>SE of regression 1.13957 R-squared -0.00069 Log-lik.(n=8120)
-10985.79738</p>
<p>Later, in Section @ref(subsec:gets:sp500:example), we will simplify
this model with the <code>getsv</code> function. For now, we provide a
comparison with a log-GARCH(1, 1) using the R package
<strong>lgarch</strong>, see <span class="citation">(<a href="#ref-sucarrat2015b">Sucarrat 2015</a>)</span>. The following code
loads the package, estimates the model and stores the estimation
results:</p>
<p>library(“lgarch”) sp500Mod02 &lt;- lgarch(sp500Ret)</p>
<p>Extracting the log-likelihood by <code>logLik(sp500Mod02)</code>
reveals that it is substantially lower, namely
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>11396.11</mn></mrow><annotation encoding="application/x-tex">-11396.11</annotation></semantics></math>.
To compare the models in terms of the <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz 1978</a>)</span> information
criterion, it is necessary to undertake the comparison in terms of the
average log-likelihoods, since the estimation samples of the two models
have a different number of observations:</p>
<p>info.criterion(as.numeric(logLik(sp500Mod01)), n = 8120, k = 17)
info.criterion(as.numeric(logLik(sp500Mod02)), n = 8240, k = 3)</p>
<p>The value increases from 2.7247 in <code>sp500Mod01</code> to 2.7693
in <code>sp500Mod02</code>, which indicates that the former
specification provides a better fit.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:gets:model:selection">GETS modeling<a class="anchor" aria-label="anchor" href="#sec:gets:model:selection"></a>
</h2>
<div class="section level3">
<h3 id="subsec:getsm:modeling:the:mean">getsm(): Modeling the mean<a class="anchor" aria-label="anchor" href="#subsec:getsm:modeling:the:mean"></a>
</h3>
<p>GETS modeling of the mean specification in a regression (e.g., a
simple time series or cross-sectional model) is undertaken by applying
the <code>getsm</code> function on an ‘<code>arx</code>’ object. This
conducts GETS variable selection on the regressors included in the
initially specified <code>arx</code> model. For example, the following
code performs GETS model selection on the regressors of the mean
specification of <code>mod05</code> with default values on all the
optional arguments:</p>
<p>getsm05 &lt;- getsm(mod05)</p>
<p>The results are stored in an object named <code>getsm05</code>, and
the information produced during the specification search is:</p>
<p>GUM mean equation:</p>
<p>reg.no. keep coef std.error t-stat p-value<br>
mconst 1 0 -0.0596894 0.0782285 -0.7630 0.4475<br>
ar1 2 0 0.1938157 0.1235456 1.5688 0.1202<br>
ar2 3 0 0.0343803 0.1141559 0.3012 0.7640<br>
mxreg1 4 0 0.1171045 0.0805838 1.4532 0.1496<br>
mxreg2 5 0 0.0116124 0.0865925 0.1341 0.8936<br>
mxreg3 6 0 -0.1087162 0.0815946 -1.3324 0.1861<br>
mxreg4 7 0 -0.2226722 0.1019820 -2.1834 0.0316 mxreg5 8 0 0.0012498
0.0694024 0.0180 0.9857</p>
<p>GUM log-variance equation:</p>
<p>coef std.error t-stat p-value<br>
vconst 0.351872 0.438687 0.6434 0.42249<br>
arch1 0.268975 0.107470 2.5028 0.01424 arch2 0.088540 0.159135 0.5564
0.57941<br>
arch3 0.022932 0.115861 0.1979 0.84357<br>
asym2 -0.112941 0.171767 -0.6575 0.51262<br>
vxreg1 0.102181 0.110374 0.9258 0.35718<br>
vxreg2 -0.068873 0.093762 -0.7345 0.46464<br>
vxreg3 -0.032006 0.102597 -0.3120 0.75584<br>
vxreg4 0.029429 0.106865 0.2754 0.78369<br>
vxreg5 0.187176 0.120259 1.5564 0.12332</p>
<p>Diagnostics:</p>
<p>Chi-sq df p-value Ljung-Box AR(3) 0.18672 3 0.97970 Ljung-Box ARCH(4)
0.43983 4 0.97909</p>
<p>7 path(s) to search Searching: 1 2 3 4 5 6 7</p>
<p>Path 1: 1 8 5 3 4 6 2 Path 2: 2 8 5 3 1 4 6 Path 3: 3 8 5 1 4 6 2
Path 4: 4 3 5 8 1 6 2 Path 5: 5 8 3 1 4 6 2 Path 6: 6 8 5 3 1 4 2 Path
7: 8 5 3 1 4 6 2</p>
<p>Terminal models:</p>
<p>info(sc) logl n k spec 1 (1-cut): 2.285792 -109.7113 98 1</p>
<p>Retained regressors (final model):</p>
<p>mxreg4</p>
<p>To see the estimation results of the final model, type
<code>getsm05</code>. The first part of the printed results pertains to
the GUM, i.e. the starting model. Note in particular that regressors are
numbered (the <code>reg.no</code> column in the GUM mean equation). This
is useful when interpreting paths searched, which indicates in which
order the regressors are deleted in each path. Next, the
<code>Terminal models</code> part contains the distinct terminal
specifications. By default, the <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz 1978</a>)</span> information
criterion (sc) is used to choose among the terminals, but this can be
changed (see below). The last part contains the estimation results of
the final, simplified model.</p>
<p>The main optional arguments of the <code>getsm</code> function are
(type <code>args(getsm)</code> or <code><a href="../reference/getsm.html">?getsm</a></code> for all the
arguments):</p>
<ul>
<li><p><code>t.pval</code>: numeric value between 0 and 1 (The default
is 0.05). The significance level used for the two-sided
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-tests
of the regressors.</p></li>
<li><p><code>wald.pval</code>: numeric value between 0 and 1 (the
default is <code>t.pval</code>). The significance level used for the
parsimonious encompassing test (PET) against the general unrestricted
model (GUM) at each regressor deletion.</p></li>
<li><p><code>do.pet</code>: logical, <code>TRUE</code> (the default) or
<code>FALSE</code>. If <code>TRUE</code>, then a PET against the GUM is
undertaken at each regressor removal.</p></li>
<li><p><code>ar.LjungB</code>: a list with two elements named
<code>lag</code> and <code>pval</code>, respectively, or
<code>NULL</code>. If the list is not <code>NULL</code>, then a <span class="citation">(<a href="#ref-ljung1978measure">Ljung and Box
1978</a>)</span> test for serial correlation in the standardized
residuals is undertaken at each attempt to remove a regressor. The
default, <code>list(lag = NULL, pval = 0.025)</code>, means the lag is
chosen automatically (as <code>max(ar) + 1</code>), and that a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>~value
of <code>pval = 0.025</code> is used. If the list is <code>NULL</code>,
then the standardized residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>z</mi><mo accent="true">̂</mo></mover><mi>t</mi></msub><annotation encoding="application/x-tex">\widehat{z}_t</annotation></semantics></math>
are not checked for serial correlation after each removal.</p></li>
<li><p><code>arch.LjungB</code>: a list with two elements named
<code>lag</code> and <code>pval</code>, respectively, or
<code>NULL</code>. If the list is not <code>NULL</code>, then a <span class="citation">(<a href="#ref-ljung1978measure">Ljung and Box
1978</a>)</span> test for serial correlation in the squared standardized
residuals is undertaken at each attempt to remove a regressor. The
default, <code>list(lag = NULL, pval = 0.025)</code>, means the lag is
chosen automatically (as <code>max(arch) + 1</code>) and that a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>~value
of <code>pval = 0.025} is used. If the list is</code>NULL`, then the
squared standardized residuals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mover><mi>z</mi><mo accent="true">̂</mo></mover><mi>t</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">\widehat{z}_t^2</annotation></semantics></math>
are not checked for serial correlation after each removal.</p></li>
<li><p><code>vcov.type</code>: <code>NULL</code>,
<code>"ordinary"</code>, <code>"white"</code> or
<code>"newey-west"</code>. If <code>NULL</code> (default), then the type
of variance-covariance matrix is automatically determined (the option
from the `<code>arx</code>’ object is used). If <code>"ordinary"</code>,
then the ordinary variance-covariance matrix is used. If
<code>"white"</code>, then the variance-covariance matrix of <span class="citation">(<a href="#ref-white1980heteroskedasticity">White
1980</a>)</span> is used. If <code>"newey-west"</code>, then the
variance-covariance matrix of <span class="citation">(<a href="#ref-newey1987simple">Newey and West 1987</a>)</span> is
used.</p></li>
<li><p><code>keep</code>: either <code>NULL</code> or an integer vector.
If <code>NULL</code> (default), then no regressors are excluded from
removal. Otherwise, the regressors associated with the numbers in
<code>keep</code> are excluded from the removal space. For example,
<code>keep = 1</code> excludes the intercept from removal. Retaining
variables using the <code>keep</code> argument implements the
“theory-embedding” approach outlined in <span class="citation">(<a href="#ref-hendry2015model">Hendry and Johansen 2015</a>)</span> by
“forcing” theory variables to be retained while conducting model
discovery beyond the set of forced variables.</p></li>
<li><p><code>info.method</code>: <code>"sc"</code>, <code>"aic"</code>
or <code>"hq"</code>. If <code>"sc"</code> (default), then the
information criterion of <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz 1978</a>)</span> is used as
tiebreaker between the terminals. If <code>"aic"</code>, then the
information criterion of <span class="citation">(<a href="#ref-Akaike1974">Akaike 1974</a>)</span> is used, and if
<code>"hq"</code>, then the information criterion of <span class="citation">(<a href="#ref-hannan1979determination">Hannan and
Quinn 1979</a>)</span> is used.</p></li>
</ul>
<p>As an example, the following code uses a lower significance level for
the regressor significance tests and the PETs, and turns of diagnostic
testing for ARCH in the standardized residuals:</p>
<p>getsm05a &lt;- getsm(mod05, t.pval = 0.01, arch.LjungB = NULL)</p>
<p>Similarly, the following code restricts the mean intercept from being
deleted, even though it is not significant:</p>
<p>getsm05b &lt;- getsm(mod05, keep = 1)</p>
</div>
<div class="section level3">
<h3 id="subsec:getsv:modeling:the:log-variance">getsv(): Modeling the log-variance}<a class="anchor" aria-label="anchor" href="#subsec:getsv:modeling:the:log-variance"></a>
</h3>
<p>GETS modeling of the log-variance specification is undertaken by
applying the <code>getsv</code> function to an ‘<code>arx</code>’
object. For example, the following code performs GETS model selection of
the log-variance specification of <code>mod05</code> with default values
on all the optional arguments:</p>
<p>getsv05 &lt;- getsv(mod05)</p>
<p>Alternatively, the following code undertakes GETS model selection on
the log-variance specification of the simplified model
<code>getsm05</code>:</p>
<p>mod06 &lt;- arx(residuals(getsm05), mc = FALSE, arch = 1:3, asym = 2,
vxreg = log(mX^2)) getsv06 &lt;- getsv(mod06)</p>
<p>Typing <code>getsv06</code> prints the results. Note that
<code>vconst</code>, the log-variance intercept, is forced to enter the
<code>keep</code> set when <code>getsv</code> is used. That is,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>α</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\alpha_0</annotation></semantics></math>
is restricted from removal even if it is not significant. This is due to
the estimation procedure, which is via the AR-representation. Finally,
the main optional arguments of <code>getsv</code> are almost the same as
those of <code>getsm</code> (see above). The main difference is that the
only variance-covariance matrix available is the ordinary one, since the
error-term of the AR-specification is iid. As an example of how to set
some of the options to non-default values, the following code restricts
the three log-ARCH terms (in addition to the log-variance intercept)
from removal, and turns off diagnostic testing for serial correlation in
the standardized residuals:</p>
<p>getsv06b &lt;- getsv(mod06, keep = 1:4, ar.LjungB = NULL)</p>
</div>
<div class="section level3">
<h3 id="subsec:gets:extraction:functions">Extraction functions<a class="anchor" aria-label="anchor" href="#subsec:gets:extraction:functions"></a>
</h3>
<p>There are a number of extraction functions available for
<code>gets</code> objects, i.e., objects produced by either
<code>getsm</code> or <code>getsv</code>. The most important functions
(most of them 3 methods) are:</p>
<p><code>coef, ES, fitted, logLik, paths, plot, predict, print, recursive,   residuals, rsquared, sigma, summary, terminals, toLatex, VaR, vcov</code></p>
<p>All, apart from <code>paths</code> and <code>terminals</code>, behave
in a similar way to the corresponding extraction functions for
‘<code>arx</code>’ objects. In particular, <code>coef</code>,
<code>fitted</code>, <code>print</code> and <code>residuals</code>
automatically detect whether <code>getsm</code> or <code>getsv</code>
has been used, and behave accordingly. The <code>paths</code> function
extracts the paths searched, and <code>terminals</code> the terminal
models.</p>
</div>
<div class="section level3">
<h3 id="subsec:gets:inflation:example">Example: A parsimonious model of quarterly inflation<a class="anchor" aria-label="anchor" href="#subsec:gets:inflation:example"></a>
</h3>
<p>In Section @ref(subsec:arx:example), we showed that a log-ARCH(4)-X
specification of the log-variance improved the fit and diagnostics of an
AR(4)-X model of quarterly inflation. Here, we obtain a simplified
version by using the <code>getsm</code> and <code>getsv</code>
functions.</p>
<p>The estimation results of the AR(4)-X-log-ARCH(4)-X specification
that we fitted was stored as an ‘<code>arx</code>’ object named
<code>inflMod02</code>. The following code undertakes GETS modeling of
the mean, and stores the results in an object named
<code>inflMod03</code>:</p>
<p>inflMod03 &lt;- getsm(inflMod02)</p>
<p>This produces the following during the specification search:</p>
<p>GUM mean equation:</p>
<p>reg.no. keep coef std.error t-stat p-value<br>
mconst 1 0 0.8386311 0.2961338 2.8319 0.005637 ar1 2 0 0.7257550
0.1300407 5.5810 2.211e-07 ar2 3 0 0.0195911 0.1171347 0.1673
0.867523<br>
ar3 4 0 0.0350092 0.1385735 0.2526 0.801087<br>
ar4 5 0 -0.1676751 0.1336972 -1.2541 0.212836<br>
q2dum 6 0 -0.0148892 0.2333917 -0.0638 0.949266<br>
q3dum 7 0 -0.0072972 0.2262704 -0.0322 0.974340<br>
q4dum 8 0 0.0103990 0.2226772 0.0467 0.962849</p>
<p>GUM log-variance equation:</p>
<p>coef std.error t-stat p-value<br>
vconst 0.95935 0.53464 3.2199 0.072749 arch1 0.16697 0.10352 1.6130
0.110169<br>
arch2 0.12027 0.10335 1.1637 0.247566<br>
arch3 0.14740 0.10332 1.4267 0.157060<br>
arch4 0.05982 0.10515 0.5689 0.570824<br>
q2dum -1.32860 0.61862 -2.1477 0.034366 q3dum -0.92707 0.58400 -1.5874
0.115843<br>
q4dum -1.82736 0.62014 -2.9467 0.004069</p>
<p>Diagnostics:</p>
<p>Chi-sq df p-value Ljung-Box AR(5) 9.1776 5 0.10219 Ljung-Box ARCH(5)
1.7613 5 0.88109</p>
<p>6 path(s) to search Searching: 1 2 3 4 5 6</p>
<p>Path 1: 3 7 6 8 4 5 -5 Path 2: 4 7 6 8 3 5 -5 Path 3: 5 7 6 3 8 -8 4
-4 Path 4: 6 7 8 3 4 5 -5 Path 5: 7 6 8 3 4 5 -5 Path 6: 8 7 6 3 4 5
-5</p>
<p>Terminal models:</p>
<p>info(sc) logl n k spec 1: 1.722352 -82.59571 104 3 spec 2: 1.776284
-83.07798 104 4</p>
<p>Retained regressors (final model):</p>
<p>mconst ar1 ar4</p>
<p>In addition to the intercept, the final model contains the AR(1) and
AR(4) terms, but no quarterly dummies. So the level of quarterly
year-on-year inflation does not seem to depend on quarter. Note that, in
the searched paths, regressor no. 5 (i.e., the AR(4) term) has a minus
sign in front of it in all but one of the searched paths. This means the
term has been re-introduced after deletion, since its deletion leads to
a violation of one or several of the diagnostics tests. This is the
reason the AR(4) term is retained even though it is not significant in
the final model. Next, we use the residuals of the simplified model to
develop a parsimonious model of the log-variance, storing the results in
<code>inflMod05</code>:</p>
<p>inflMod04 &lt;- arx(residuals(inflMod03), mc = FALSE, arch = 1:4,
vxreg = inflData[, 2:4]) inflMod05 &lt;- getsv(inflMod04, ar.LjungB =
list(lag = 5, pval = 0.025))</p>
<p>Note that, to ensure that the diagnostic test for autocorrelation in
the standardized residuals is undertaken at the same lag as earlier, the
<code>ar.LjungB</code> argument has been modified. Next, typing
<code>inflMod05</code> prints the results, and again we only reproduce
selected parts in the interest of brevity:</p>
<p>SPECIFIC log-variance equation:</p>
<p>coef std.error t-stat p-value<br>
vconst 0.71311 0.53965 1.7462 0.186355<br>
arch1 0.17438 0.10057 1.7339 0.086217 arch2 0.16822 0.10034 1.6764
0.096975 q2dum -1.43834 0.62992 -2.2834 0.024662 q3dum -1.09189 0.60035
-1.8187 0.072135 q4dum -1.82836 0.60351 -3.0295 0.003163</p>
<p>Diagnostics and fit:</p>
<p>Chi-sq df p-value Ljung-Box AR(5) 8.1224 5 0.1496 Ljung-Box ARCH(5)
7.7418 5 0.1711</p>
<p>The results suggest a high impact of the ARCH(1) and ARCH(2) terms –
much higher than for financial returns,<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;In finance, if
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;msub&gt;&lt;mi&gt;ϵ&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;/msub&gt;&lt;annotation encoding="application/x-tex"&gt;\epsilon_t&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;
is a mean-corrected financial return, then the ARCH(1) term is usually
about 0.05, and almost never higher than 0.1.&lt;/p&gt;'><sup>9</sup></a> and that the
conditional variance depends on quarter. To obtain an idea of the
economic importance of our results, we re-estimate the full, simplified
model, and generate out-of-sample forecasts of the conditional standard
deviation up to four quarters ahead. The full, simplified model is
re-estimated using:</p>
<p>inflMod06 &lt;- inflMod06 &lt;- arx(inflData[, “infl”], ar = c(1, 4),
arch = 1:2, vxreg = inflData[, 2:4], vcov.type = “white”)</p>
<p>In order to generate out-of-sample forecasts, we first need to
generate the out-of-sample values of the retained quarterly dummies:</p>
<p>newvxreg &lt;- matrix(0, 4, 3) colnames(newvxreg) &lt;- c(“q2dum”,
“q3dum”, “q4dum”) newvxreg[2, “q2dum”] &lt;- 1 newvxreg[3, “q3dum”]
&lt;- 1 newvxreg[4, “q4dum”] &lt;- 1</p>
<p>We can now generate the out-of-sample forecasts of the conditional
standard deviations:</p>
<p>set.seed(123) predict(inflMod06, n.ahead = 4, spec = “variance”,
newvxreg = newvxreg)</p>
<p>The first command, <code>set.seed(123)</code>, is for reproducibility
purposes, since a bootstrap procedure is used to generate variance
forecasts two or more steps ahead (the number of draws can be changed
via the <code>n.sim</code> argument). The forecasts for 2016(1) to
2016(4) are:</p>
<p>2016(1) 2016(2) 2016(3) 2016(4) 1.0448239 0.3403215 0.4628250
0.2075531</p>
<p>In other words, the conditional variance is forecasted to be about
four times higher in 2016(1) than in 2016(4). This has notable economic
consequences. For example, if the forecasted inflation in 2016(1) is 2%,
then an approximate 95% prediction interval computed as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>±</mo><mn>2</mn><mo>×</mo><msub><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>1</mn></mrow></msub></mrow><annotation encoding="application/x-tex">2 \pm 2 \times \widehat{\sigma}_{n+1}</annotation></semantics></math>
is given by the range
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>0</mn><annotation encoding="application/x-tex">0</annotation></semantics></math>%
to 4%, which is large. By contrast, an approximate 95% prediction
interval for 2016(4) computed as
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>±</mo><mn>2</mn><mo>×</mo><msub><mover><mi>σ</mi><mo accent="true">̂</mo></mover><mrow><mi>n</mi><mo>+</mo><mn>4</mn></mrow></msub></mrow><annotation encoding="application/x-tex">2 \pm 2 \times \widehat{\sigma}_{n+4}</annotation></semantics></math>
is given by the range 1.1% to 2.9%, which is much tighter.</p>
</div>
<div class="section level3">
<h3 id="subsec:gets:sp500:example">Example: A parsimonious model of daily SP500 volatility<a class="anchor" aria-label="anchor" href="#subsec:gets:sp500:example"></a>
</h3>
<p>In Section @ref(subsec:arx:<a href="example:sp500-volatility" class="uri">example:sp500-volatility</a>) we estimated a rich model of
daily SP500 return volatility named <code>sp500Mod01</code>.
Simplification of this model is straightforward with the
<code>getsv</code> function. Since the model does not fully get rid of
the ARCH in the standardized residuals, we will turn off the ARCH
diagnostics. Also, for parsimony we will choose a small regressor
significance level equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.1</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">0.1\%</annotation></semantics></math>:</p>
<p>sp500Mod03 &lt;- getsv(sp500Mod01, t.pval = 0.001, arch.LjungB =
NULL)</p>
<p>Typing <code>sp500Mod03</code> returns (amongst other):</p>
<p>SPECIFIC log-variance equation:</p>
<p>coef std.error t-stat p-value<br>
vconst 0.016309 0.044960 0.1316 0.7167940<br>
arch1 -0.064147 0.013740 -4.6687 3.080e-06 arch5 0.038690 0.011324
3.4168 0.0006368 logEqWMA(20) 0.427071 0.053110 8.0413 1.014e-15
logEqWMA(120) 0.327148 0.052734 6.2038 5.782e-10 volproxylag 0.197866
0.036558 5.4124 6.396e-08 dWed -0.176576 0.064799 -2.7250 0.0064442</p>
<p>Diagnostics and fit:</p>
<p>Chi-sq df p-value<br>
Ljung-Box AR(1) 0.40681 1 0.5236<br>
Ljung-Box ARCH(6) 32.33070 6 1.41e-05</p>
<p>SE of regression 1.14417 Log-lik.(n=8120) -10993.62221</p>
<p>In other words, no day-of-the-week dummies are retained and only the
first ARCH-term is retained. However, three of the log-proxies are
retained, i.e., the weekly, the monthly and the half-yearly, and both
the lagged range-based volatility proxy and the lagged log-volume
difference are retained. The log-likelihood is now
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>−</mo><mn>11131.4</mn></mrow><annotation encoding="application/x-tex">-11131.4</annotation></semantics></math>,
and the following code computes the <span class="citation">(<a href="#ref-schwarz1978estimating">Schwarz 1978</a>)</span> information
criterion value in terms of the average log-likelihood: %</p>
<p>info.criterion(as.numeric(logLik(sp500Mod03)), n = 8120, k = 7)</p>
<p>The value is 2.7155, so so the parsimonious model provides a better
fit (according to sc) compared with the GUM (i.e.,
<code>sp500Mod01</code>).</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:indicator:saturation">Indicator saturation<a class="anchor" aria-label="anchor" href="#sec:indicator:saturation"></a>
</h2>
<p>Indicator saturation has been a crucial development in GETS modeling
to address the distorting influence of outliers and structural breaks
(changes in parameters) in econometric models. Such parameter changes
are generally of unknown magnitudes and may occur at unknown times.
Indicator saturation tackles this challenge by starting from a general
model allowing for an outlier or shift at every point and removing all
but significant ones using GETS selection. This serves both as a method
to detect outliers and breaks, as well as a generalized approach to
model mis-specification testing – if the model is well-specified, then
no outliers/shifts will be detected. The function <code>isat</code>
conducts multi-path indicator saturation to detect outliers and
location-shifts in regression models using impulse indicator saturation
(IIS – see , and for a comprehensive asymptotic analysis),
step-indicator saturation (SIS – see ), trend-indicator saturation (TIS
– as applied in ), and user-designed indicator saturation (UIS, or
designed break functions in , and ). Formulating the detection of
structural breaks as a problem of model selection, a regression model is
saturated with a full set of indicators which are then selected over
using the general-to-specific <code>getsm</code> algorithm at a chosen
level of significance <code>t.pval</code>. This approach to break
detection imposes no minimum break length, and outliers can be
identified jointly with structural breaks. The false-detection rate or
gauge in IS is given by
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">\alpha k</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
irrelevant indicators selected over, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k=n</annotation></semantics></math>
for IIS and TIS, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=n-1</annotation></semantics></math>
for SIS if an intercept is forced. Thus, the false-detection rate can
easily be controlled by reducing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
at the cost of reduced power of detecting true shifts and outliers. To
ensure a low false-detection rate, the rule of thumb of setting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>=</mo><mo>min</mo><mrow><mo stretchy="true" form="prefix">(</mo><mn>0.05</mn><mo>,</mo><mrow><mo stretchy="true" form="prefix">[</mo><mn>1</mn><mi>/</mi><mi>k</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\alpha=\min(0.05,[1/k])</annotation></semantics></math>
can be used, which yields one incorrectly retained indicator in
expectation for large samples, and aims for a false-detection rate below
5% in small samples. Figure @ref(fig_isat) (and Table
@ref(tab_isatgauge) in Appendix @ref(sec:simulation-tables)) show the
false-detection rate in IS using <code>isat</code> in a simple static
simulation for increasing sample sizes.</p>
<p>The respective GUMs for a simple model of the mean of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>t</mi></msub><annotation encoding="application/x-tex">y_t</annotation></semantics></math>
using impulse-, step- and trend-indicator saturation<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Note that specifications of step-functions are possible
in SIS. Here we specify the steps as in (@ref(eq_sisgum)), and thus for
interpretation every additional step is added to the previous ones. In
contrast, the paper introducing SIS &lt;span class="citation"&gt;(&lt;a href="#ref-castle2015detecting"&gt;Castle et al. 2015&lt;/a&gt;)&lt;/span&gt; works
with step-indicators of the form
&lt;math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"&gt;&lt;semantics&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi&gt;δ&lt;/mi&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;/msub&gt;&lt;msub&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mrow&gt;&lt;mo stretchy="false" form="prefix"&gt;{&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;≤&lt;/mo&gt;&lt;mi&gt;j&lt;/mi&gt;&lt;mo stretchy="false" form="postfix"&gt;}&lt;/mo&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;annotation encoding="application/x-tex"&gt;\sum^{n}_{j=2} \delta_j 1_{\{ t \leq j \}}&lt;/annotation&gt;&lt;/semantics&gt;&lt;/math&gt;,
in which case the steps have to be subtracted from the previous sum of
steps to interpret the coefficients.&lt;/p&gt;'><sup>10</sup></a> are given by</p>
<p>SIS GUM:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>μ</mi><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>2</mn></mrow><mi>n</mi></msubsup><msub><mi>δ</mi><mi>j</mi></msub><msub><mn>1</mn><mrow><mo stretchy="false" form="prefix">{</mo><mi>t</mi><mo>≥</mo><mi>j</mi><mo stretchy="false" form="postfix">}</mo></mrow></msub><mo>+</mo><msub><mi>u</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t = \mu + \sum^{n}_{j=2} \delta_j 1_{\{ t \geq j \} } + u_t</annotation></semantics></math></p>
<p>IIS GUM:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>μ</mi><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>δ</mi><mi>j</mi></msub><msub><mn>1</mn><mrow><mo stretchy="false" form="prefix">{</mo><mi>t</mi><mo>=</mo><mi>j</mi><mo stretchy="false" form="postfix">}</mo></mrow></msub><mo>+</mo><msub><mi>u</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t = \mu + \sum^{n}_{j=1} \delta_j 1_{\{ t = j \} } + u_t</annotation></semantics></math></p>
<p>TIS GUM:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>y</mi><mi>t</mi></msub><mo>=</mo><mi>μ</mi><mo>+</mo><msubsup><mo>∑</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><msub><mi>δ</mi><mi>j</mi></msub><msub><mn>1</mn><mrow><mo stretchy="false" form="prefix">{</mo><mi>t</mi><mo>&gt;</mo><mi>j</mi><mo stretchy="false" form="postfix">}</mo></mrow></msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>t</mi><mo>−</mo><mi>j</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>+</mo><msub><mi>u</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">y_t = \mu + \sum^{n}_{j=1} \delta_j 1_{\{ t &gt; j \} }(t-j) + u_t</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
denotes the total number of observations in the sample. Indicators are
partitioned into blocks based on the values of the
<code>ratio.threshold</code> and <code>max.block.size</code> arguments
of the <code>isat</code> function, where the block size used is the
maximum of given by either criterion. Indicators retained in each block
are re-combined and selected over to yield terminal models. Additional
regressors that are not selected over can be included through the
<code>mxreg</code> argument, where autoregressive terms in particular,
can be included using the <code>ar</code> argument. Naturally different
indicators can be combined, by specifying both <code>iis = TRUE</code>
and <code>sis = TRUE</code> selection takes place over both impulse- as
well as step-indicators. The different regimes made up of indicators
(e.g., retained step-functions or impulses) weighted by their estimated
coefficients describe shifts in the intercept over time – the
coefficient path of the intercept. While the detection of shifts in SIS
is focused on time-series analysis, IIS can be used in cross-sectional
regression models to detect individual outliers (see e.g., ).</p>
<p>The primary arguments for selection of indicators in
<code>isat</code> carry over from the <code>getsm</code> function. The
main differences and additional arguments are: %</p>
<ul>
<li><p><code>t.pval</code>: numeric value between 0 and 1. The
significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>
used for the two-sided
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-tests
of the indicators in selection. The default is lower than in regular
<code>getsm</code> model selection and set to 0.001 to control the
number of false positives. Under the null of no outliers (or structural
breaks), the irrelevance proportion or gauge (or proportion of
spuriously retained indicators) is equal to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">\alpha k</annotation></semantics></math>
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of indicators selected over. Thus setting
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mo>≈</mo><mn>1</mn><mi>/</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">\alpha \approx 1/k</annotation></semantics></math>
yields one spuriously retained indicator on average under the
null.</p></li>
<li><p><code>iis</code>: logical, <code>TRUE</code> or
<code>FALSE</code>. If <code>TRUE</code>, then a full set of impulse
indicators is added and selected over.</p></li>
<li><p><code>sis</code>: logical, <code>TRUE</code> or
<code>FALSE</code>. If <code>TRUE</code>, then a full set of step
indicators is added and selected over.</p></li>
<li><p><code>tis</code>: logical, <code>TRUE</code> or
<code>FALSE</code>. If <code>TRUE</code>, then a full set of trend
indicators is added and selected over.</p></li>
<li><p><code>uis</code>: matrix object that contains designed break
functions to be selected over.</p></li>
<li><p><code>ratio.threshold</code>: numeric, between 0 and 1. Minimum
ratio of variables in each block to total observations to determine the
block size, default equals 0.8. Block size used is the maximum of given
by either the <code>ratio.threshold</code> and
<code>max.block.size</code>.</p></li>
<li><p><code>max.block.size</code>: an integer of at least 2. Maximum
size of block of variables to be selected over, default equals 30. Block
size used is the maximum of given by either the
<code>ratio.threshold</code> and <code>max.block.size</code>.</p></li>
<li><p><code>parallel.options</code>: either <code>NULL</code> or an
integer. The integer denotes the number of cores to be used to search
over blocks in parallel. If the argument is <code>NULL</code> then no
parallel computation is used. This option can speed up computation when
the number of blocks of indicators to be searched over is
large.</p></li>
</ul>
<div class="section level3">
<h3 id="example-structural-breaks-in-the-growth-rate-of-uk-so_2-emissions">Example: Structural breaks in the growth rate of UK
SO<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mn>2</mn></msub><annotation encoding="application/x-tex">_2</annotation></semantics></math>
emissions<a class="anchor" aria-label="anchor" href="#example-structural-breaks-in-the-growth-rate-of-uk-so_2-emissions"></a>
</h3>
<p>Annual emissions of the pollutant sulphur dioxide
(SO<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mn>2</mn></msub><annotation encoding="application/x-tex">_2</annotation></semantics></math>)
in the UK have declined in the latter half of the 20th century due to
policy interventions and changes in energy production. Here we assess
whether there have been significant shifts in the growth rate
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Δ</mi><mo>log</mo><msub><mrow><mo stretchy="true" form="prefix">(</mo><mi>S</mi><msub><mi>O</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\Delta \log(SO_2)_t</annotation></semantics></math>)
of sulphur dioxide emissions between 1946 and 2005, using SIS and the
emission time series compiled by <span class="citation">(<a href="#ref-smith2011anthropogenic">Smith et al. 2011</a>)</span>.
Setting <code>t.pval</code> to 0.01 yields an approximate gauge of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.01</mn><mi>k</mi></mrow><annotation encoding="application/x-tex">0.01k</annotation></semantics></math>
under the null hypothesis of no shifts for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
spuriously included variables. Inclusion of a full set of indicators
implies that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">k=n</annotation></semantics></math>
for IIS, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><mi>n</mi><mo>−</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k=n-1</annotation></semantics></math>
for SIS, and thus
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>0.01</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>−</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>0.01</mn><mo>×</mo><mn>59</mn></mrow><annotation encoding="application/x-tex">0.01(n-1) = 0.01 \times 59</annotation></semantics></math>.
This suggests less than one indicator being retained spuriously on
average in a well-specified model under the null of no shifts or
outliers. Estimating an <code>isat</code> model using SIS
(<code>sis = TRUE</code> is default): %</p>
<p>options(plot = TRUE) so2 &lt;- data(“so2data”, package =
“gets”)<br>
yso2 &lt;- zoo(so2data[, “DLuk_tot_so2”], order.by = so2data[, “year”])
(sis &lt;- isat(yso2, t.pval = 0.01))</p>
<p>SIS block 1 of 2: 30 paths to search Searching: 1 2 3 4 …</p>
<p>SIS block 2 of 2:</p>
<p>26 paths to search Searching: 1 2 3 4 …</p>
<p>GETS of union of retained SIS variables… 2 paths to search Searching:
1 2</p>
<p>…</p>
<p>SPECIFIC mean equation:</p>
<p>coef std.error t-stat p-value mconst 0.01465385 0.007931984 1.847438
7.026836e-02 sis1972 -0.04332051 0.011866458 -3.650669 5.990412e-04
sis1993 -0.11693333 0.020126141 -5.810023 3.625832e-07 sis1998
0.12860000 0.044305650 2.902564 5.382516e-03 sis1999 -0.28400000
0.057198348 -4.965178 7.505854e-06 sis2000 0.24550000 0.045219264
5.429102 1.441154e-06 sis2004 -0.11550000 0.035026692 -3.297485
1.746083e-03</p>
<p>Diagnostics:</p>
<p>Chi-sq df p-value Ljung-Box AR(1) 0.61553 1 0.43271 Ljung-Box ARCH(1)
1.44153 1 0.22989 Jarque-Bera 0.57302 2 0.75088</p>
<p>SE of regression 0.04045 R-squared 0.73021 Log-lik.(n=60)
110.83192</p>
<p>The above output shows multiple detected step-shifts (labeled
<code>sis1972</code>–<code>sis2004</code>) in the time series. If
plotting is active (<code>plot = TRUE</code>), <code>isat</code> also
displays the output as in Figure @ref(fig_sisso2) plotting the observed
and fitted values, together with the coefficient path (the time-varying
intercept through the regimes detected using SIS) as well as the
standardized residuals. There is a downward step-shift detected in the
growth rate in 1972, outlying observations are detected through two
subsequent step-indicators with opposite-signs (e.g., in 1998/1999), as
well as a downward step-shift at the end of the sample in 2004. This
example demonstrates the flexibility of the SIS approach – step-shifts
are easily identified even at the end of the sample while outliers can
be detected simultaneously. The model can easily be extended to include
autoregressive terms using the <code>ar</code> argument, for example we
could estimate an AR(1) model with step-indicator saturation writing
<code>isat(yso2, ar = 1, t.pval = 0.01)</code>. Detection of outliers
and structural breaks can be directly parallelized to increase
computational speed when there are a large number of blocks searched
over by setting the argument <code>parallel.options</code> equal to the
number of cores available for processing. For example,
<code>isat(yso2, t.pval = 0.01, parallel.options = 2)</code> estimates
the above model in parallel using two cores.</p>
<p>Additional covariates can be included in an IS regression model by
including them in the <code>mxreg</code> argument. If fixed regressors
entering through <code>mxreg</code> induce perfect collinear with break
functions in IS, then indicators are removed automatically before
selection. For example, consider forcing a hypothesized step-shift in
1972 to be retained while simultaneously searching for additional shifts
throughout the sample: %</p>
<p>x1972 &lt;- zoo(sim(so2data[, “year”])[, 26], order.by = so2data[,
“year”]) isat(yso2, t.pval = 0.01, mxreg = x1972)</p>
<p>The resulting estimation does not select over the fixed step-shift in
1972, though for this particular example the estimated terminal model
with a forced step shift matches the SIS results of a general search.
%</p>
</div>
<div class="section level3">
<h3 id="testing-and-bias-correcting-post-model-selection-in-indicator-saturation">Testing and bias correcting post-model selection in indicator
saturation<a class="anchor" aria-label="anchor" href="#testing-and-bias-correcting-post-model-selection-in-indicator-saturation"></a>
</h3>
<p>The coefficient path describes how the value of a coefficient on a
particular variable evolves over time. The coefficient path of the
intercept of the ‘<code>isat</code>’ object can be extracted using the
<code>isatvar</code> function. The function returns the coefficient path
both as the time-varying intercept (<code>const.path</code>) and as
deviation relative to the full-sample intercept
(<code>coef.path</code>), together with the approximate variance of the
coefficient path computed using the approach in <span class="citation">(<a href="#ref-pretis2017classifying">Felix Pretis
2017</a>)</span>. When the model is specified to include autoregressive
terms, then <code>isatvar</code> (setting <code>lr = TRUE</code>) also
returns the static long-run solution of the dynamic model with its
approximate variance. %</p>
<p>sisvar &lt;- isatvar(sis) sisvar</p>
<p>coef.path const.path const.var const.se 1946 0.00000000 0.01465385
6.291637e-05 0.007931984 1947 0.00000000 0.01465385 6.291637e-05
0.007931984 …</p>
<p>Indicator saturation may result in an under-estimation of the error
variance as observations are ``dummied out’’ resulting in a truncation
of the distribution of the error terms. The magnitude of this effect
depends on the level of significance of selection and is generally small
for low values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>α</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
This effect manifests itself in an under-estimation of the error
variance, and an under-estimation of the variance of regressors not
selected over. Both can be corrected when using IIS through consistency
and efficiency correction factors derived in <span class="citation">(<a href="#ref-johansen2016asymptotic">Johansen and Nielsen
2016</a>)</span>. These correction factors are implemented in
<strong>gets</strong> as functions <code>isvarcor</code> which corrects
the estimated error variance, and <code>isvareffcor</code> for an
additional correction on the estimated variance of fixed regressors. The
correction factors can be applied manually to estimation results, or
specified as arguments (<code>conscorr = TRUE</code> and
<code>effcorr = TRUE</code>) within the <code>isatvar</code> function.
This is demonstrated below running IIS on an autoregressive model with
one lag (<code>ar = 1</code>) on the growth rate of
SO<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mn>2</mn></msub><annotation encoding="application/x-tex">_2</annotation></semantics></math>
emissions. The estimated variance of the coefficient path is higher once
consistency and efficiency corrections are applied: %</p>
<p>iis &lt;- isat(yso2, ar = 1, sis = FALSE, iis = TRUE, t.pval = 0.05)
isatvar(iis, conscorr = TRUE, effcorr = TRUE)</p>
<p>coef.path const.path const.var const.se 1947 0.00000000 -0.006210179
7.225479e-05 0.008500282 1948 0.00000000 -0.006210179 7.225479e-05
0.008500282 …</p>
<p>isatvar(iis, conscorr = FALSE, effcorr = FALSE)</p>
<p>coef.path const.path const.var const.se 1947 0.00000000 -0.006210179
4.483453e-05 0.006695859 1948 0.00000000 -0.006210179 4.483453e-05
0.006695859 …</p>
<p>The terminal models of <code>isat</code> are the result of model
selection, and may therefore lead to a selection bias in the coefficient
estimates of selected variables. Post-selection bias-correction for
orthogonal variables can be conducted using the method proposed in <span class="citation">(<a href="#ref-hendry2005properties">Hendry and Krolzig
2005</a>)</span>. This is implemented as the function
<code>biascorr</code>. Following <span class="citation">(<a href="#ref-pretis2017classifying">Felix Pretis 2017</a>)</span>,
bias-correction of the coefficients in a SIS model can be directly
applied to the coefficient path without prior orthogonalization.
Bias-correcting the coefficient path of the above model of the growth
rate of
SO<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mn>2</mn></msub><annotation encoding="application/x-tex">_2</annotation></semantics></math>
yields the one- and two-step bias-corrected coefficients: %</p>
<p>bcorr &lt;- biascorr(b = sisvar[, “const.path”], b.se = sisvar[,
“const.se”], p.alpha = 0.01, T = length(sisvar[, “const.path”]))</p>
<p>beta beta.1step beta.2step … 1997 -0.14560000 -0.14560000 -0.14560000
1998 -0.01700000 -0.01700000 -0.01700000 1999 -0.30100000 -0.30099983
-0.30099983 2000 -0.05550000 -0.04043232 -0.03000334 2001 -0.05550000
-0.04043232 -0.03000334 …</p>
<p>Bias-correction reduces the magnitude of the estimated coefficients
slightly to account for potential selection bias.</p>
<p>The function <code>isattest</code> makes it possible to conduct
hypothesis tests on the coefficient path of the intercept of an
`<code>isat</code>’ object. This test is described in <span class="citation">(<a href="#ref-pretis2017classifying">Felix Pretis
2017</a>)</span> and builds on <span class="citation">(<a href="#ref-ericsson2013biased">Ericsson and Hendry 2013</a>)</span> and
<span class="citation">(<a href="#ref-pretis2015testing">Felix Pretis,
Mann, and Kaufmann 2015</a>)</span> who use indicator saturation as a
test for time-varying forecast accuracy. The main arguments of the
<code>isattest</code> function are: %</p>
<ul>
<li><p><code>hnull</code>: numeric. The null-hypothesis value to be
tested against.</p></li>
<li><p><code>lr</code>: logical. If <code>TRUE</code> and the
`<code>isat</code>’ object to be tested contains autoregressive terms,
then the test is conducted on the long-run equilibrium coefficient
path.</p></li>
<li><p><code>ci.pval</code>: numeric, between 0 and 1. The level of
significance for the confidence interval and hypothesis test.</p></li>
<li><p><code>conscorr</code>: logical. If <code>TRUE</code> then the
estimated error variance in IIS is consistency-corrected using the
results in <span class="citation">(<a href="#ref-johansen2016asymptotic">Johansen and Nielsen
2016</a>)</span>.</p></li>
<li><p><code>effcorr</code>: logical. If <code>TRUE</code> then the
estimated variance of fixed regressors in IIS is efficiency corrected
using the results in <span class="citation">(<a href="#ref-johansen2016asymptotic">Johansen and Nielsen
2016</a>)</span>.</p></li>
<li><p><code>biascorr</code>: logical. If <code>TRUE</code> then the
coefficient path is bias-corrected prior to testing. This is only valid
for a non-dynamic (no auto-regressive terms) test without additional
covariates.</p></li>
</ul>
<p>Here we test the time-varying mean (as determined using SIS) of the
annual growth rate of UK
SO<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi></mi><mn>2</mn></msub><annotation encoding="application/x-tex">_2</annotation></semantics></math>
emissions against the null hypothesis of zero-growth using
<code>isattest</code>: %</p>
<p>isattest(sis, hnull = 0, lr = FALSE, ci.pval = 0.99, plot.turn =
TRUE, biascorr = TRUE)</p>
<p>ci.low ci.high bias.high bias.low 1946 -0.006539007 0.035846700 0
0.0000000 1947 -0.006539007 0.035846700 0 0.0000000 1948 -0.006539007
0.035846700 0 0.0000000 …</p>
<p>The results are shown in the automatically-generated plot given in
Figure @ref(fig_sistest) (the <code>plot.turn = TRUE</code> argument
automatically adds the break dates into the plot in the lower panel).
When testing at 1% and using bias-correction this suggests that the
detected shift in 1972 does not significantly move the growth-rate away
from zero. Similarly, the upward shift in 2000 moves the growth rate
back to zero. This change, however, is off-set by the shift at the end
of the sample which shows the growth rate turning significantly negative
in 2004.</p>
</div>
<div class="section level3">
<h3 id="comparison-of-isat-with-other-methods-isat_comp">Comparison of isat with other methods {isat_comp}<a class="anchor" aria-label="anchor" href="#comparison-of-isat-with-other-methods-isat_comp"></a>
</h3>
<p>Indicator saturation formulates the detection of breaks and outliers
as a problem of model (or variable) selection. Here we first provide an
overview of software implementing indicator saturation, followed by a
discussion of <code>isat</code> in <strong>gets</strong> relative to
other existing break detection packages.</p>
<p>The only other currently existing software implementing indicator
saturation is <strong>Autometrics</strong>. IIS and SIS are available in
both <strong>Autometrics</strong> and <strong>gets</strong>, however, a
crucial difference within SIS is the construction and subsequent
interpretation of step-indicators. In <strong>gets</strong> steps are
constructed as forward-steps: <span class="math inline">${\bf S} =
\left( 1_{\{t \geq j\}}, j=1,...,n\right)$</span>, where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mn>1</mn><mrow><mo stretchy="false" form="prefix">{</mo><mi>t</mi><mo>≥</mo><mi>j</mi><mo stretchy="false" form="postfix">}</mo></mrow></msub><annotation encoding="application/x-tex">1_{\{t \geq j \}}</annotation></semantics></math>
denotes the indicator function such that
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mn>1</mn><mrow><mo stretchy="false" form="prefix">{</mo><mi>t</mi><mo>≥</mo><mi>j</mi><mo stretchy="false" form="postfix">}</mo></mrow></msub><mo>=</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">1_{ \{t \geq j\}}=1</annotation></semantics></math>
for observations from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
onwards, and zero otherwise. Thus the signs of the coefficients in the
retained regression model in <strong>gets</strong> correspond to the
direction of the step: positive (negative) coefficients imply an upward
(downward) step, and the coefficient path begins with the regression
intercept where for each subsequent regime the coefficient on the step
indicator is added to the full sample intercept.
<strong>Autometrics</strong> relies on backward-steps: <span class="math inline">${\bf S} = \left( 1_{\{t \leq j\}},
j=1,...,n\right)$</span> and thus step-coefficients have to be
interpreted as opposite-signed relative to the reported regression
coefficients. <strong>Autometrics</strong> currently has no
implementation of the computation of the coefficient path and its
approximate variance, thus testing on the different regimes is
non-trivial. This is directly implemented in <strong>gets</strong> by
automatically plotting the coefficient path (if
<code>plot = TRUE</code>), which can be extracted using
<code>isatvar</code>. The variance estimates in
<strong>Autometrics</strong> are currently not consistency or efficiency
corrected when using IS. This is implemented in <strong>gets</strong>
and – together with the extraction of the coefficient path and its
variance – enables testing on the coefficient path using the
<code>isattest</code> function, together with automatic bias-correction
if specified. Further, automatic trend-indicator saturation (TIS) is
currently only available in <strong>gets</strong>. Both
<strong>Autometrics</strong> and <strong>gets</strong> enable the
selection over designed break functions – through the argument
<code>uis</code> in <strong>gets</strong> and the general more variables
than observations model selection algorithm in
<strong>Autometrics</strong>.</p>
<p>In the broader field of detection of breaks or changepoints, the main
difference to existing methods (e.g., , , implemented in
<strong>strucchange</strong> by ) or detection of changepoints in
general (as in the package <strong>changepoint</strong> – see ) is the
model-selection approach to break detection in indicator saturation (for
discussion of methodological differences see , as well as , and ). This
makes it possible to detect outliers (single period shifts) jointly with
structural breaks (multiple period shifts), further it is also possible
to detect breaks using designed functions which is not possible in
conventional structural break methods or changepoint analysis.</p>
<p>Where the indicator saturation methodology overlaps in applications
with existing methods is the detection of shifts in the intercept of
time series regression models, for example using
<code>breakpoints</code> in <strong>strucchange</strong> . Relative to
<strong>strucchange</strong> and the Bai and Perron least-squares
approach in changes in the mean, <code>isat</code> in
<strong>gets</strong> does not impose a minimum break length and can
therefore conduct outlier detection jointly with shifts in the
intercept, further there is no implicit upper limit on the number of
breaks, and it is thus possible to identify outliers or shifts in the
mean at the start or end of samples as no trimming is required. Changes
in regression coefficients on random variables can be detected in
<code>isat</code> using designed break functions through the
<code>uis</code> argument by interacting a full set of step-indicators
with the random variable. This, however, is computationally expensive as
each additional variable whose coefficient is allowed to break over time
adds a set of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
variables to be selected over the GUM. The function
<code>breakpoints</code> in <strong>strucchange</strong> estimates a
pure structural change model where all coefficients change,
<code>isat</code> in <strong>gets</strong> is a partial model where the
coefficients on variables included through <code>mxreg</code> are not
allowed to break, and only breaks in the mean (or specified coefficients
through inclusion in <code>uis</code>) are permitted – making it
possible to pre-specify constancy. A partial structural change model
using the Bai and Perron least-squares approach can be estimated using
available code.<a class="footnote-ref" tabindex="0" data-bs-toggle="popover" data-bs-content='&lt;p&gt;Available online at &lt;a href="http://people.bu.edu/perron/code.html" class="external-link uri"&gt;http://people.bu.edu/perron/code.html&lt;/a&gt;.&lt;/p&gt;'><sup>11</sup></a></p>
<p>Relative to <strong>changepoint</strong> , <code>isat</code> in
<strong>gets</strong> is focused on regression modeling and structural
breaks in the intercept of regression models jointly with outlier
detection. As the authors of <strong>changepoint</strong> themselves
note, <strong>changepoint</strong> does not focus on changes in
regression models. The function <code>isat</code> directly enables the
inclusion of covariates through <code>mxreg</code> or <code>ar</code>
within <code>isat</code>, only if no additional covariates are specified
then <code>isat</code> searches for changes in the mean of a time series
as in the models used in the <strong>changepoint</strong> package while,
however, simultaneously detecting outliers.</p>
</div>
</div>
<div class="section level2">
<h2 id="sec:eviews:and:stata:export">Exporting results to EViews, STATA and LaTeX<a class="anchor" aria-label="anchor" href="#sec:eviews:and:stata:export"></a>
</h2>
<p>The two most popular commercial econometric software packages are
EViews and STATA , but none of these provide GETS modeling capabilities.
To facilitate the usage of GETS modeling for EViews and STATA users, we
provide two functions for this purpose, <code>eviews</code> and
<code>stata</code>. Both functions work in a similar way, and both can
be applied on either ‘<code>arx</code>’,‘<code>gets</code>’ or
‘<code>isat</code>’ objects. For example, typing
<code>eviews(getsm05)</code> yields the following print output: %</p>
<p>EViews code to estimate the model:</p>
<p>equation getsm05.ls(cov = white) yy mxreg4</p>
<p>R code (example) to export the data of the model:</p>
<p>eviews(getsm05, file = ‘C:/Users/myname/Documents/getsdata.csv’)</p>
<p>In other words, the code to estimate the final model in EViews, and –
if needed – a code-suggestion for how to export the data of the model.
The need to export the data of the final model is likely to be most
relevant subsequent to the use of <code>isat</code>. The
<code>stata</code> function works similarly. Note that both the
<code>eviews</code> and <code>stata</code> functions are only applicable
to conditional mean specifications, since neither EViews nor STATA offer
the estimation of dynamic log-variance models.</p>
<p>The objects returned by <code>arx</code>, <code>getsm</code>,
<code>getsv</code> and <code>isat</code> are lists. The entries in these
lists that contain the main estimation output are objects of class
`<code>data.frame</code>’. That means the R package
<strong>xtable</strong> of <span class="citation">(<a href="#ref-Dahl2016">Dahl 2016</a>)</span> can be used to generate code
of the data frames.</p>
</div>
<div class="section level2">
<h2 id="sec:conclusions">Conclusions<a class="anchor" aria-label="anchor" href="#sec:conclusions"></a>
</h2>
<p>The R package <strong>gets</strong> provides a toolkit for
general-to-specific modeling through automatic variable selection in
regression specifications of the mean and the variance, as well as
indicator saturation methods to detect outliers and structural breaks.
Starting with a general candidate set of variables unknown to be
relevant or irrelevant, selection using <code>getsm</code> or
<code>getsv</code> can yield parsimonious terminal models that satisfy a
set of chosen diagnostic criteria, where parameter changes and outlying
observations are detected using <code>isat</code>.</p>
</div>
<div class="section level2">
<h2 class="unnumbered" id="references">References<a class="anchor" aria-label="anchor" href="#references"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Akaike1974" class="csl-entry">
Akaike, H. 1974. <span>“A New Look at the Statistical Model
Identification.”</span> <em>IEEE Transactions on Automatic Control</em>
19 (6): 716–23. <a href="https://doi.org/10.1109/tac.1974.1100705" class="external-link">https://doi.org/10.1109/tac.1974.1100705</a>.
</div>
<div id="ref-CamposEricssonHendry2005" class="csl-entry">
Campos, J., D. F. Hendry, and N. R. Ericsson, eds. 2005.
<em>General-to-Specific Modeling. Volumes 1 and 2</em>. Cheltenham:
Edward Elgar Publishing.
</div>
<div id="ref-castle2011evaluating" class="csl-entry">
Castle, Jennifer L, Jurgen A Doornik, and David F Hendry. 2011.
<span>“Evaluating Automatic Model Selection.”</span> <em>Journal of Time
Series Econometrics</em> 3 (1): 1–31. <a href="https://doi.org/10.2202/1941-1928.1097" class="external-link">https://doi.org/10.2202/1941-1928.1097</a>.
</div>
<div id="ref-castle2015detecting" class="csl-entry">
Castle, Jennifer L, Jurgen A Doornik, David F Hendry, and Felix Pretis.
2015. <span>“Detecting Location Shifts During Model Selection by
Step-Indicator Saturation.”</span> <em>Econometrics</em> 3 (2): 240–64.
<a href="https://doi.org/10.3390/econometrics3020240" class="external-link">https://doi.org/10.3390/econometrics3020240</a>.
</div>
<div id="ref-Dahl2016" class="csl-entry">
Dahl, D. B. 2016. <em><code>xtable</code>: Export Tables to LaTeX or
HTML</em>. <a href="https://CRAN.R-project.org/package=xtable" class="external-link">https://CRAN.R-project.org/package=xtable</a>.
</div>
<div id="ref-Doornik2009" class="csl-entry">
Doornik, J. 2009. <span>“<code>Autometrics</code>.”</span> In <em>The
Methodology and Practice of Econometrics: A Festschrift in Honour of
David f. Hendry</em>, edited by J. L. Castle and N. Shephard, 88–121.
Oxford: Oxford University Press.
</div>
<div id="ref-Duan1983" class="csl-entry">
Duan, N. 1983. <span>“Smearing Estimate: A Nonparametric
Retransformation Method.”</span> <em>Journal of the Americal Statistical
Association</em> 78 (383): 605–10. <a href="https://doi.org/10.2307/2288126" class="external-link">https://doi.org/10.2307/2288126</a>.
</div>
<div id="ref-Engle82" class="csl-entry">
Engle, R. 1982. <span>“Autoregressive Conditional Heteroscedasticity
with Estimates of the Variance of United Kingdom Inflations.”</span>
<em>Econometrica</em> 50 (4): 987–1008. <a href="https://doi.org/10.2307/1912773" class="external-link">https://doi.org/10.2307/1912773</a>.
</div>
<div id="ref-ericsson2013biased" class="csl-entry">
Ericsson, N. R., and D. F. Hendry. 2013. <span>“Biased Regressors and
Spurious Regressions.”</span> <em>Journal of Econometrics</em> 177 (2):
357–65. <a href="https://doi.org/10.1016/j.jeconom.2013.06.004" class="external-link">https://doi.org/10.1016/j.jeconom.2013.06.004</a>.
</div>
<div id="ref-glosten1993relation" class="csl-entry">
Glosten, Lawrence R, Ravi Jagannathan, and David E Runkle. 1993.
<span>“On the Relation Between the Expected Value and the Volatility of
the Nominal Excess Return on Stocks.”</span> <em>Journal of Finance</em>
48 (5): 1779–1801. <a href="https://doi.org/10.2307/2329067" class="external-link">https://doi.org/10.2307/2329067</a>.
</div>
<div id="ref-hannan1979determination" class="csl-entry">
Hannan, Edward J, and Barry G Quinn. 1979. <span>“The Determination of
the Order of an Autoregression.”</span> <em>Journal of the Royal
Statistical Society B</em> 41 (2): 190–95.
</div>
<div id="ref-hendry2003sargan" class="csl-entry">
Hendry, David F. 2003. <span>“J. Denis Sargan and the Origins of LSE
Econometric Methodology.”</span> <em>Econometric Theory</em> 19 (3):
457–80. <a href="https://doi.org/10.1017/s0266466603193061" class="external-link">https://doi.org/10.1017/s0266466603193061</a>.
</div>
<div id="ref-hendry2015model" class="csl-entry">
Hendry, David F, and Søren Johansen. 2015. <span>“Model Discovery and
Trygve Haavelmo’s Legacy.”</span> <em>Econometric Theory</em> 31 (1):
93–114. <a href="https://doi.org/10.1017/s0266466614000218" class="external-link">https://doi.org/10.1017/s0266466614000218</a>.
</div>
<div id="ref-hendry2008automatic" class="csl-entry">
Hendry, David F, Søren Johansen, and Carlos Santos. 2008.
<span>“Automatic Selection of Indicators in a Fully Saturated
Regression.”</span> <em>Computational Statistics</em> 23 (2): 317–35. <a href="https://doi.org/10.1007/s00180-007-0054-z" class="external-link">https://doi.org/10.1007/s00180-007-0054-z</a>.
</div>
<div id="ref-hendry2005properties" class="csl-entry">
Hendry, David F, and Hans-Martin Krolzig. 2005. <span>“The Properties of
Automatic Gets Modelling.”</span> <em>The Economic Journal</em> 115
(502): C32–61. <a href="https://doi.org/10.1111/j.0013-0133.2005.00979.x" class="external-link">https://doi.org/10.1111/j.0013-0133.2005.00979.x</a>.
</div>
<div id="ref-hoover1999data" class="csl-entry">
Hoover, Kevin D, and Stephen J Perez. 1999. <span>“Data Mining
Reconsidered: Encompassing and the General-to-Specific Approach to
Specification Search.”</span> <em>The Econometrics Journal</em> 2 (2):
167–91. <a href="https://doi.org/10.1111/1368-423x.00025" class="external-link">https://doi.org/10.1111/1368-423x.00025</a>.
</div>
<div id="ref-JarqueBera1980" class="csl-entry">
Jarque, C., and A. Bera. 1980. <span>“Efficient Tests for Normality,
Homoscedasticity, and Serial Independence of Regression
Residuals.”</span> <em>Econometrica</em> 48 (6): 1519–32. <a href="https://doi.org/10.2307/1912352" class="external-link">https://doi.org/10.2307/1912352</a>.
</div>
<div id="ref-johansen2016asymptotic" class="csl-entry">
Johansen, Søren, and Bent Nielsen. 2016. <span>“Asymptotic Theory of
Outlier Detection Algorithms for Linear Time Series Regression
Models.”</span> <em>Scandinavian Journal of Statistics</em> 43 (2):
321–48. <a href="https://doi.org/10.1111/sjos.12174" class="external-link">https://doi.org/10.1111/sjos.12174</a>.
</div>
<div id="ref-ljung1978measure" class="csl-entry">
Ljung, Greta M, and George EP Box. 1978. <span>“On a Measure of Lack of
Fit in Time Series Models.”</span> <em>Biometrika</em> 65 (2): 297–303.
<a href="https://doi.org/10.2307/2335207" class="external-link">https://doi.org/10.2307/2335207</a>.
</div>
<div id="ref-lovell1983data" class="csl-entry">
Lovell, Michael C. 1983. <span>“Data Mining.”</span> <em>The Review of
Economics and Statistics</em> 65 (1): 1–12. <a href="https://doi.org/10.2307/1924403" class="external-link">https://doi.org/10.2307/1924403</a>.
</div>
<div id="ref-mizon1995progressive" class="csl-entry">
Mizon, Grayham. 1995. <span>“Progressive Modeling of Macroeconomic Time
Series: The LSE Methodology.”</span> In <em>Macroeconometrics.
Developments, Tensions and Prospects</em>, edited by Kevin D Hoover,
107–69. Dordrecht: Kluwer Academic Publishers.
</div>
<div id="ref-newey1987simple" class="csl-entry">
Newey, Whitney K, and Kenneth D West. 1987. <span>“A Simple Positive
Semi-Definite, Heteroskedasticity and Autocorrelation Consistent
Covariance Matrix.”</span> <em>Econometrica</em> 55 (3): 703–8. <a href="https://doi.org/10.2307/1913610" class="external-link">https://doi.org/10.2307/1913610</a>.
</div>
<div id="ref-pretis2017classifying" class="csl-entry">
Pretis, Felix. 2017. <span>“Classifying Time-Varying Predictive Accuracy
in Using Bias-Corrected Indicator Saturation.”</span>
</div>
<div id="ref-pretis2015testing" class="csl-entry">
Pretis, Felix, Michael L Mann, and Robert K Kaufmann. 2015.
<span>“Testing Competing Models of the Temperature Hiatus: Assessing the
Effects of Conditioning Variables and Temporal Uncertainties Through
Sample-Wide Break Detection.”</span> <em>Climatic Change</em> 131 (4):
705–18. <a href="https://doi.org/10.1007/s10584-015-1391-5" class="external-link">https://doi.org/10.1007/s10584-015-1391-5</a>.
</div>
<div id="ref-pretis2018automated" class="csl-entry">
Pretis, Felix, J James Reade, and Genaro Sucarrat. 2018.
<span>“Automated General-to-Specific (GETS) Regression Modeling and
Indicator Saturation for Outliers and Structural Breaks.”</span>
</div>
<div id="ref-pretis_volc16" class="csl-entry">
Pretis, F., and U. Volz. 2016. <span>“Nowcasting the Swiss
Economy.”</span> <em>Swiss Journal of Economics and Statistics</em> 152
(2): 95–135. <a href="https://doi.org/10.1007/BF03399588" class="external-link">https://doi.org/10.1007/BF03399588</a>.
</div>
<div id="ref-schwarz1978estimating" class="csl-entry">
Schwarz, Gideon. 1978. <span>“Estimating the Dimension of a
Model.”</span> <em>The Annals of Statistics</em> 6 (2): 461–64. <a href="https://doi.org/10.1214/aos/1176344136" class="external-link">https://doi.org/10.1214/aos/1176344136</a>.
</div>
<div id="ref-smith2011anthropogenic" class="csl-entry">
Smith, Steven J, John Van Aardenne, Zbigniew Klimont, Robert J Andres,
Anja Volke, and Sabine Delgado Arias. 2011. <span>“Anthropogenic Sulfur
Dioxide Emissions: 1850–2005.”</span> <em>Atmospheric Chemistry and
Physics</em> 11 (3): 1101–16. <a href="https://doi.org/10.5194/acp-11-1101-2011" class="external-link">https://doi.org/10.5194/acp-11-1101-2011</a>.
</div>
<div id="ref-sucarrat2010econometric" class="csl-entry">
Sucarrat, Genaro. 2010. <span>“Econometric Reduction Theory and
Philosophy.”</span> <em>Journal of Economic Methodology</em> 17 (1):
53–75. <a href="https://doi.org/10.1080/13501780903528978" class="external-link">https://doi.org/10.1080/13501780903528978</a>.
</div>
<div id="ref-sucarrat2015b" class="csl-entry">
———. 2015. <em>Lgarch: Simulation and Estimation of Log-GARCH
Models</em>. <a href="https://CRAN.R-project.org/package=lgarch" class="external-link">https://CRAN.R-project.org/package=lgarch</a>.
</div>
<div id="ref-sucarrat2012automated" class="csl-entry">
Sucarrat, Genaro, and Ángel Escribano. 2012. <span>“Automated Model
Selection in Finance: General-to-Specific Modelling of the Mean and
Volatility Specifications.”</span> <em>Oxford Bulletin of Economics and
Statistics</em> 74 (5): 716–35. <a href="https://doi.org/10.1111/j.1468-0084.2011.00669.x" class="external-link">https://doi.org/10.1111/j.1468-0084.2011.00669.x</a>.
</div>
<div id="ref-sucarrat2016estimation" class="csl-entry">
Sucarrat, Genaro, Steffen Grønneberg, and Ángel Escribano. 2016.
<span>“Estimation and Inference in Univariate and Multivariate
Log-GARCH-x Models When the Conditional Density Is Unknown.”</span>
<em>Computational Statistics &amp; Data Analysis</em> 100: 582–94. <a href="https://doi.org/10.1016/j.csda.2015.12.005" class="external-link">https://doi.org/10.1016/j.csda.2015.12.005</a>.
</div>
<div id="ref-white1980heteroskedasticity" class="csl-entry">
White, Halbert. 1980. <span>“A Heteroskedasticity-Consistent Covariance
Matrix and a Direct Test for Heteroskedasticity.”</span>
<em>Econometrica</em> 48 (4): 817–38. <a href="https://doi.org/10.2307/1912934" class="external-link">https://doi.org/10.2307/1912934</a>.
</div>
<div id="ref-zeileis2005zoo" class="csl-entry">
Zeileis, Achim, and Gabor Grothendieck. 2005. <span>“Zoo: S3
Infrastructure for Regular and Irregular Time Series.”</span>
<em>Journal of Statistical Software</em> 14 (6): 1–27. <a href="https://doi.org/10.18637/jss.v014.i06" class="external-link">https://doi.org/10.18637/jss.v014.i06</a>.
</div>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="appendix-appendix">(APPENDIX) Appendix<a class="anchor" aria-label="anchor" href="#appendix-appendix"></a>
</h2>
</div>
<div class="section level2">
<h2 id="hoover-and-perez-hoover1999data-simulations">Hoover and Perez <span class="citation">(<a href="#ref-hoover1999data">Hoover and Perez 1999</a>)</span>
simulations<a class="anchor" aria-label="anchor" href="#hoover-and-perez-hoover1999data-simulations"></a>
</h2>
<p>Table @ref(table:list 1 of experiments) contains the list of
experiments. The design of the experiments HP1, HP2’ and HP7’ are based
on , and make use of their data
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>x</mi><mrow><mn>1</mn><mi>t</mi></mrow><mtext mathvariant="normal">HP</mtext></msubsup><mo>,</mo><mi>.</mi><mi>.</mi><mi>.</mi><mo>,</mo><msubsup><mi>x</mi><mrow><mn>36</mn><mi>t</mi></mrow><mtext mathvariant="normal">HP</mtext></msubsup></mrow><annotation encoding="application/x-tex">x_{1t}^{\text{HP}},...,x_{36t}^{\text{HP}}</annotation></semantics></math>.
It should be noted that there are two typos in their Table 3. The value
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msqrt><mrow><mo stretchy="true" form="prefix">(</mo><mn>7</mn><mi>/</mi><mn>4</mn><mo stretchy="true" form="postfix">)</mo></mrow></msqrt><annotation encoding="application/x-tex">\sqrt{(7/4)}</annotation></semantics></math>
should instead be
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mn>7</mn></msqrt><mi>/</mi><mn>4</mn></mrow><annotation encoding="application/x-tex">\sqrt{7}/4</annotation></semantics></math>
in the model of the autoregressive error, and the value 6.73 should
instead be 6.44 in model 7’, see also . The number of relevant variables
in the GUM is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><annotation encoding="application/x-tex">k_{\text{rel}}</annotation></semantics></math>,
the number of irrelevant variables in the GUM is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">k_{\text{irr}}</annotation></semantics></math>
and the total number of variables (the constant included) in the GUM is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>=</mo><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo>+</mo><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k = k_{\text{rel}} + k_{\text{irr}} + 1</annotation></semantics></math>.
Nominal regressor significance level used is 5%, and 1000 replications.
The term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">rel</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\hat{k}_{\text{rel}}/k_{\text{rel}})</annotation></semantics></math>
is the average proportion of relevant variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">rel</mtext></msub><annotation encoding="application/x-tex">\hat{k}_{\text{rel}}</annotation></semantics></math>
retained relative to the actual number of relevant variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">rel</mtext></msub><annotation encoding="application/x-tex">k_{\text{rel}}</annotation></semantics></math>
in the DGP. The term
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><mi>/</mi><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">m(\hat{k}_{\text{irr}}/k_{\text{irr}})</annotation></semantics></math>
denotes the average proportion of irrelevant variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">\hat{k}_{\text{irr}}</annotation></semantics></math>
retained relative to the actual number of irrelevant variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>k</mi><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">k_{\text{irr}}</annotation></semantics></math>
in the GUM. The estimate
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>k</mi><mo accent="true">̂</mo></mover><mtext mathvariant="normal">irr</mtext></msub><annotation encoding="application/x-tex">\hat{k}_{\text{irr}}</annotation></semantics></math>
includes both significant and insignificant retained irrelevant
variables (this is in line with , and , but counter to HP which only
includes significant irrelevant variables in the estimate).
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>p</mi><mo accent="true">̂</mo></mover><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">DGP</mtext><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\hat{p}(\text{DGP})</annotation></semantics></math>
is the proportion of times the DGP is found exactly. The properties of
the HP algorithm are from . The properties of the
<strong>PcGets</strong> algorithm are from , and the properties of the
Autometrics algorithm are from . For <strong>AutoSEARCH</strong>, a
constant is included in all the regressions but ignored in the
evaluation of successes and failures (this is in line with but counter
to , and ), in the diagnostic checks both the AR and ARCH test of the
standardized residuals were undertaken at lag 2 using a significance
level of 2.5% for each, and as tiebreaker the Schwarz information
criterion is used with a Gaussian log-likelihood made up of the
standardized residuals of the mean specification.</p>
</div>
<div class="section level2">
<h2 id="sec:simulation-tables">Simulation tables<a class="anchor" aria-label="anchor" href="#sec:simulation-tables"></a>
</h2>
<p>Tables @ref(tab_lassuncorr), @ref(tab_lassposcorr),
@ref(tab_lassnegcorr) and @ref(tab_isatgauge) present the simulation
results comparing <strong>gets</strong> to alternative variable
selection methods, and the properties of <code>isat</code> under the
null of no breaks.</p>
</div>

  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Genaro Sucarrat.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.0.</p>
</div>

    </footer>
</div>





  </body>
</html>
